{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4c7a23d",
   "metadata": {},
   "source": [
    "# Variational Inference: Spam Detection\n",
    "\n",
    "In this assignment, we will load the UCI SMS Spam Collection dataset.\n",
    "However, instead of using it directly, we will use fixed-size vector embeddings of the message.\n",
    "Those embeddings have already been produced and are provided here to you.\n",
    "\n",
    "\n",
    "## Setting the Scene\n",
    "\n",
    "The goal of this assignment to go beyond a traditional classifier and apply variational methods.\n",
    "This means that we will train a model using some parameters over the distribution of which we have a prior belief.\n",
    "Our prior belief here is actually that each and every parameter independently follows a standard normal distribution.\n",
    "\n",
    "\n",
    "During optimization, we will draw sets of parameters from our variational distribution $q_{\\phi}$.\n",
    "Drawing these parameters needs to be done using the reparameterization trick, so that we can add noise, i.e., $w_i=\\mu_i+\\sigma_i\\cdot\\epsilon$.\n",
    "The assignment also poses one or the other question (clearly marked), you need to provide your answer directly after.\n",
    "\n",
    "\n",
    "The assignment has some blank spots for you to fill out (but you can customize your implementation to your liking).\n",
    "Training should be done using stochastic gradient descend, either using manual gradient updates or using an optimizer.\n",
    "\n",
    "You should use autodiff-capabilities to compute gradients.\n",
    "It is recommended to use, for example, JAX or PyTorch.\n",
    "We recommend the latter, using it in a functional way (i.e., using `torch.func.jacrev`).\n",
    "The blanks left in this assignment and their type hints assume PyTorch.\n",
    "There are some cells with quick tests/sanity-checks, that you are free to remove, especially if they do not go along with how you chose to implement your solution.\n",
    "\n",
    "\n",
    "At the end of the assignment, after training, you need to pick one advanced method of evaluation.\n",
    "We are not interested in traditional metrics here (e.g., accuracy, Kappa, F1, etc.; although you are welcome to show those).\n",
    "Rather, we want to exploit the variational nature of the model here and show something more interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0cb7f4",
   "metadata": {},
   "source": [
    "# Load the Data\n",
    "\n",
    "You're variational model shall use no more than 15 components (aim for ~5 or fewer).\n",
    "You'll have to apply a dimensionality reduction.\n",
    "\n",
    "Sentence Embeddings were created in two ways:\n",
    "\n",
    "1. (Recommended) Using [ALBERT XLarge v2](https://huggingface.co/albert/albert-xlarge-v2) (`albert-xlarge-v2`). Dim $=2,048$.\n",
    "2. Using [English word vectors](https://fasttext.cc/docs/en/english-vectors.html) from `wiki-news-300d-1M` using `fasttext`. Dim $=300$.\n",
    "\n",
    "In either case, the embeddings were averaged along the sequence dimension to produce fixed-size vectors.\n",
    "In the provided dataset, the messages are retained.\n",
    "This is useful should you choose a qualitative evaluation.\n",
    "\n",
    "The labels have already been converted to floats: 0.0=ham, 1.0=spam.\n",
    "The default example below shows a spam message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8da9d2e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5572, 2048),\n",
       " (5572,),\n",
       " 5572,\n",
       " np.str_(\"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\"),\n",
       " np.int64(1))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modify only the file you wish to load.\n",
    "# Each data file contains 3 keys: X, y, msg\n",
    "import numpy as np\n",
    "\n",
    "data = np.load(file='2048d_sms_spam_albert-xlarge-v2.npz')\n",
    "# data = np.load(file='300d_sms_spam_fasttext_pca.npz')\n",
    "\n",
    "X: np.ndarray; Y: np.ndarray\n",
    "X, Y, msg = data.get('X'), data.get('y'), data.get('msg')\n",
    "X.shape, Y.shape, len(msg), msg[2], Y[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aeb3353",
   "metadata": {},
   "source": [
    "# Define the Model\n",
    "\n",
    "Our model is to be a polynomial with degree corresponding to the number of components chosen for the PCA.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    z=w_0+w_1\\cdot x_1+w_2x_2^2+\\dots+w_nx_n^n.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We will perform a **binary** classification problem, using the binary cross-entropy (CE).\n",
    "CE has a range of $[0,\\infty)$.\n",
    "The better the predictions of our model, the lower the CE.\n",
    "\n",
    "\n",
    "Note that binary CE requires our predictions to be in the range $[0,1]$.\n",
    "Therefore, we will have to pass its raw outputs (\"logits\") through the Sigmoid function.\n",
    "This makes our model a **logistic** classifier.\n",
    "The Sigmoid function is defined as $s(x)=\\frac{1}{1+e^{-z}}$.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    p_i&=s(z_i),\\;\\text{convert our raw predictions to probabilities, then:}\n",
    "    \\\\[1ex]\n",
    "    \\hat{y}_i&\\sim\\text{Bernoulli}(p_i).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "Since our predicted $\\hat{y}$ will follow a Bernoulli distribution, we can directy use its likelihood function.\n",
    "Note that maximizing the Bernoulli likelihood is equivalent of minimizing the binary CE!\n",
    "The Bernoulli distribution is parameterized by a single parameter, **$p$**.\n",
    "\n",
    "\n",
    "However, in our context, $p$ is unknown.\n",
    "In order not to confuse the Bernoulli distribution's parameter $p$ with anything, in the following, we have substituted it with $s(z_i)$.\n",
    "We will optimize for it, so that the output of our logisitic classifier becomes $p$.\n",
    "The likelihood (for the prediction $\\hat{y}_i$ of a single observation and its label $y_i$) then becomes:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    p(\\mathbf{y}|\\mathbf{X},\\mathbf{w})&=\\prod_i\\,s(z_i)^{y_i}\\cdot(1-s(z_i))^{1-y_i},\n",
    "    \\\\\n",
    "    \\log{(p(\\mathbf{y}|\\mathbf{X},\\mathbf{w}))}&=\\sum_i\\,\\left[\\log{(s(z_i)^{y_i})}+\\log{((1-s(z_i))^{1-y_i})}\\right],\n",
    "    \\\\\n",
    "    &=\\sum_i\\,\\left[y_i\\cdot\\log{(s(z_i))} + (1-y_i)\\cdot\\log{(1-s(z_i))}\\right].\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "* Our **prior** belief is that each parameter follows a standard normal distribution, i.e., $w_j\\sim\\mathcal{N}(0,1)$.\n",
    "* For our variational distribution, use a mean-field approximation of standard (independent) normals, too.\n",
    "\n",
    "\n",
    "**Question**: Conceptually, how does the log-likelihood $p(y|x,w)$ compute its result given a single $n$-dimensional observation under the assumption of independent dimensions?\n",
    "\n",
    "**Answer**: In case of independent dimensions, the (log-)likelihood across all dimensions is multiplied (summed)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8eaf94",
   "metadata": {},
   "source": [
    "# Implement the Model\n",
    "\n",
    "Here, you are encouraged to use a library/framework like PyTorch or JAX and esp. functionality for automatic differentiation to compute gradients.\n",
    "You may also import functions like `sigmoid` or `vmap` for vectorized operations.\n",
    "\n",
    "It is recommended to implement vectorized versions of your required functions (i.e., batch-processing).\n",
    "\n",
    "It is preferable to use type-hints for your functions.\n",
    "Furthermore, you can write better code by inserting assertions (e.g., for dimensionality or other sanity-checks).\n",
    "Please use Python-style comments like in the following:\n",
    "\n",
    "```python\n",
    "def func(w0: float, x: float) -> float:\n",
    "    \"\"\"\n",
    "    Function to compute a multiple of x.\n",
    "    \"\"\"\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecc070fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\kemal\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.7.0-cp310-cp310-win_amd64.whl (212.5 MB)\n",
      "     ---------------------------------------- 0.0/212.5 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.2/212.5 MB 6.9 MB/s eta 0:00:31\n",
      "     --------------------------------------- 1.9/212.5 MB 24.4 MB/s eta 0:00:09\n",
      "      -------------------------------------- 2.7/212.5 MB 21.9 MB/s eta 0:00:10\n",
      "      -------------------------------------- 3.4/212.5 MB 19.6 MB/s eta 0:00:11\n",
      "      -------------------------------------- 4.0/212.5 MB 18.1 MB/s eta 0:00:12\n",
      "      -------------------------------------- 4.5/212.5 MB 16.8 MB/s eta 0:00:13\n",
      "      -------------------------------------- 4.5/212.5 MB 16.8 MB/s eta 0:00:13\n",
      "      -------------------------------------- 4.5/212.5 MB 16.8 MB/s eta 0:00:13\n",
      "      -------------------------------------- 5.1/212.5 MB 12.4 MB/s eta 0:00:17\n",
      "      -------------------------------------- 5.1/212.5 MB 12.4 MB/s eta 0:00:17\n",
      "     - ------------------------------------- 7.4/212.5 MB 14.8 MB/s eta 0:00:14\n",
      "     - ------------------------------------- 7.8/212.5 MB 14.7 MB/s eta 0:00:14\n",
      "     - ------------------------------------- 7.8/212.5 MB 14.7 MB/s eta 0:00:14\n",
      "     - ------------------------------------- 7.8/212.5 MB 14.7 MB/s eta 0:00:14\n",
      "     - ------------------------------------- 9.7/212.5 MB 14.1 MB/s eta 0:00:15\n",
      "     - ------------------------------------ 10.1/212.5 MB 14.0 MB/s eta 0:00:15\n",
      "     - ------------------------------------ 10.1/212.5 MB 14.0 MB/s eta 0:00:15\n",
      "     -- ----------------------------------- 11.6/212.5 MB 13.6 MB/s eta 0:00:15\n",
      "     -- ----------------------------------- 12.2/212.5 MB 13.1 MB/s eta 0:00:16\n",
      "     -- ----------------------------------- 12.8/212.5 MB 12.9 MB/s eta 0:00:16\n",
      "     -- ----------------------------------- 13.5/212.5 MB 12.8 MB/s eta 0:00:16\n",
      "     -- ----------------------------------- 14.1/212.5 MB 12.8 MB/s eta 0:00:16\n",
      "     -- ----------------------------------- 14.8/212.5 MB 14.6 MB/s eta 0:00:14\n",
      "     -- ----------------------------------- 15.4/212.5 MB 16.0 MB/s eta 0:00:13\n",
      "     -- ----------------------------------- 16.0/212.5 MB 14.9 MB/s eta 0:00:14\n",
      "     -- ----------------------------------- 16.6/212.5 MB 14.2 MB/s eta 0:00:14\n",
      "     --- ---------------------------------- 17.3/212.5 MB 13.4 MB/s eta 0:00:15\n",
      "     --- ---------------------------------- 17.9/212.5 MB 13.1 MB/s eta 0:00:15\n",
      "     --- ---------------------------------- 18.5/212.5 MB 14.6 MB/s eta 0:00:14\n",
      "     --- ---------------------------------- 19.1/212.5 MB 13.6 MB/s eta 0:00:15\n",
      "     --- ---------------------------------- 19.8/212.5 MB 13.4 MB/s eta 0:00:15\n",
      "     --- ---------------------------------- 20.3/212.5 MB 14.5 MB/s eta 0:00:14\n",
      "     --- ---------------------------------- 21.0/212.5 MB 13.9 MB/s eta 0:00:14\n",
      "     --- ---------------------------------- 21.6/212.5 MB 13.1 MB/s eta 0:00:15\n",
      "     --- ---------------------------------- 22.2/212.5 MB 13.1 MB/s eta 0:00:15\n",
      "     ---- --------------------------------- 22.8/212.5 MB 13.1 MB/s eta 0:00:15\n",
      "     ---- --------------------------------- 23.4/212.5 MB 13.1 MB/s eta 0:00:15\n",
      "     ---- --------------------------------- 23.5/212.5 MB 13.1 MB/s eta 0:00:15\n",
      "     ---- --------------------------------- 23.5/212.5 MB 13.1 MB/s eta 0:00:15\n",
      "     ---- --------------------------------- 24.4/212.5 MB 12.4 MB/s eta 0:00:16\n",
      "     ---- --------------------------------- 25.6/212.5 MB 13.1 MB/s eta 0:00:15\n",
      "     ---- --------------------------------- 25.6/212.5 MB 13.1 MB/s eta 0:00:15\n",
      "     ---- --------------------------------- 27.0/212.5 MB 13.1 MB/s eta 0:00:15\n",
      "     ---- --------------------------------- 27.5/212.5 MB 13.1 MB/s eta 0:00:15\n",
      "     ---- --------------------------------- 27.8/212.5 MB 12.8 MB/s eta 0:00:15\n",
      "     ----- -------------------------------- 28.9/212.5 MB 13.1 MB/s eta 0:00:15\n",
      "     ----- -------------------------------- 29.1/212.5 MB 12.6 MB/s eta 0:00:15\n",
      "     ----- -------------------------------- 29.7/212.5 MB 12.6 MB/s eta 0:00:15\n",
      "     ----- -------------------------------- 30.4/212.5 MB 12.6 MB/s eta 0:00:15\n",
      "     ----- -------------------------------- 31.0/212.5 MB 12.6 MB/s eta 0:00:15\n",
      "     ----- -------------------------------- 31.4/212.5 MB 12.4 MB/s eta 0:00:15\n",
      "     ----- -------------------------------- 31.9/212.5 MB 12.1 MB/s eta 0:00:15\n",
      "     ----- -------------------------------- 32.3/212.5 MB 12.1 MB/s eta 0:00:15\n",
      "     ----- -------------------------------- 32.8/212.5 MB 11.9 MB/s eta 0:00:16\n",
      "     ----- -------------------------------- 33.3/212.5 MB 11.7 MB/s eta 0:00:16\n",
      "     ------ ------------------------------- 33.8/212.5 MB 13.1 MB/s eta 0:00:14\n",
      "     ------ ------------------------------- 34.3/212.5 MB 12.6 MB/s eta 0:00:15\n",
      "     ------ ------------------------------- 34.8/212.5 MB 12.1 MB/s eta 0:00:15\n",
      "     ------ ------------------------------- 35.3/212.5 MB 11.7 MB/s eta 0:00:16\n",
      "     ------ ------------------------------- 35.8/212.5 MB 11.3 MB/s eta 0:00:16\n",
      "     ------ ------------------------------- 36.4/212.5 MB 11.7 MB/s eta 0:00:16\n",
      "     ------ ------------------------------- 36.9/212.5 MB 11.5 MB/s eta 0:00:16\n",
      "     ------ ------------------------------- 37.4/212.5 MB 11.1 MB/s eta 0:00:16\n",
      "     ------ ------------------------------- 38.0/212.5 MB 11.3 MB/s eta 0:00:16\n",
      "     ------ ------------------------------- 38.5/212.5 MB 10.9 MB/s eta 0:00:16\n",
      "     ------ ------------------------------- 39.1/212.5 MB 10.9 MB/s eta 0:00:16\n",
      "     ------- ------------------------------ 39.7/212.5 MB 11.1 MB/s eta 0:00:16\n",
      "     ------- ------------------------------ 40.2/212.5 MB 10.9 MB/s eta 0:00:16\n",
      "     ------- ------------------------------ 40.8/212.5 MB 10.9 MB/s eta 0:00:16\n",
      "     ------- ------------------------------ 41.4/212.5 MB 11.1 MB/s eta 0:00:16\n",
      "     ------- ------------------------------ 42.1/212.5 MB 11.5 MB/s eta 0:00:15\n",
      "     ------- ------------------------------ 42.7/212.5 MB 11.5 MB/s eta 0:00:15\n",
      "     ------- ------------------------------ 43.3/212.5 MB 11.9 MB/s eta 0:00:15\n",
      "     ------- ------------------------------ 43.8/212.5 MB 12.1 MB/s eta 0:00:14\n",
      "     ------- ------------------------------ 44.5/212.5 MB 12.3 MB/s eta 0:00:14\n",
      "     -------- ----------------------------- 45.1/212.5 MB 12.4 MB/s eta 0:00:14\n",
      "     -------- ----------------------------- 45.8/212.5 MB 12.6 MB/s eta 0:00:14\n",
      "     -------- ----------------------------- 46.5/212.5 MB 12.8 MB/s eta 0:00:13\n",
      "     -------- ----------------------------- 47.1/212.5 MB 13.1 MB/s eta 0:00:13\n",
      "     -------- ----------------------------- 47.7/212.5 MB 13.1 MB/s eta 0:00:13\n",
      "     -------- ----------------------------- 48.3/212.5 MB 13.1 MB/s eta 0:00:13\n",
      "     -------- ----------------------------- 48.8/212.5 MB 13.1 MB/s eta 0:00:13\n",
      "     -------- ----------------------------- 49.2/212.5 MB 12.8 MB/s eta 0:00:13\n",
      "     -------- ----------------------------- 49.7/212.5 MB 12.9 MB/s eta 0:00:13\n",
      "     -------- ----------------------------- 50.2/212.5 MB 12.6 MB/s eta 0:00:13\n",
      "     --------- ---------------------------- 50.7/212.5 MB 12.6 MB/s eta 0:00:13\n",
      "     --------- ---------------------------- 51.2/212.5 MB 12.4 MB/s eta 0:00:14\n",
      "     --------- ---------------------------- 51.7/212.5 MB 12.4 MB/s eta 0:00:14\n",
      "     --------- ---------------------------- 52.2/212.5 MB 12.1 MB/s eta 0:00:14\n",
      "     --------- ---------------------------- 52.7/212.5 MB 12.1 MB/s eta 0:00:14\n",
      "     --------- ---------------------------- 53.3/212.5 MB 11.9 MB/s eta 0:00:14\n",
      "     --------- ---------------------------- 53.9/212.5 MB 11.7 MB/s eta 0:00:14\n",
      "     --------- ---------------------------- 54.2/212.5 MB 11.5 MB/s eta 0:00:14\n",
      "     --------- ---------------------------- 54.8/212.5 MB 11.3 MB/s eta 0:00:14\n",
      "     --------- ---------------------------- 55.7/212.5 MB 11.3 MB/s eta 0:00:14\n",
      "     ---------- --------------------------- 56.2/212.5 MB 11.1 MB/s eta 0:00:15\n",
      "     ---------- --------------------------- 56.8/212.5 MB 10.9 MB/s eta 0:00:15\n",
      "     ---------- --------------------------- 57.4/212.5 MB 10.9 MB/s eta 0:00:15\n",
      "     ---------- --------------------------- 58.0/212.5 MB 11.1 MB/s eta 0:00:14\n",
      "     ---------- --------------------------- 58.6/212.5 MB 11.1 MB/s eta 0:00:14\n",
      "     ---------- --------------------------- 59.2/212.5 MB 11.3 MB/s eta 0:00:14\n",
      "     ---------- --------------------------- 59.8/212.5 MB 11.3 MB/s eta 0:00:14\n",
      "     ---------- --------------------------- 60.4/212.5 MB 11.5 MB/s eta 0:00:14\n",
      "     ---------- --------------------------- 61.0/212.5 MB 11.7 MB/s eta 0:00:13\n",
      "     ----------- -------------------------- 61.7/212.5 MB 11.7 MB/s eta 0:00:13\n",
      "     ----------- -------------------------- 62.6/212.5 MB 12.1 MB/s eta 0:00:13\n",
      "     ----------- -------------------------- 63.2/212.5 MB 12.4 MB/s eta 0:00:13\n",
      "     ----------- -------------------------- 63.8/212.5 MB 12.4 MB/s eta 0:00:13\n",
      "     ----------- -------------------------- 64.4/212.5 MB 12.8 MB/s eta 0:00:12\n",
      "     ----------- -------------------------- 65.1/212.5 MB 13.4 MB/s eta 0:00:12\n",
      "     ----------- -------------------------- 65.8/212.5 MB 13.4 MB/s eta 0:00:11\n",
      "     ----------- -------------------------- 66.5/212.5 MB 13.6 MB/s eta 0:00:11\n",
      "     ------------ ------------------------- 67.1/212.5 MB 13.6 MB/s eta 0:00:11\n",
      "     ------------ ------------------------- 68.1/212.5 MB 13.9 MB/s eta 0:00:11\n",
      "     ------------ ------------------------- 68.8/212.5 MB 13.9 MB/s eta 0:00:11\n",
      "     ------------ ------------------------- 69.4/212.5 MB 14.2 MB/s eta 0:00:11\n",
      "     ------------ ------------------------- 70.2/212.5 MB 14.2 MB/s eta 0:00:10\n",
      "     ------------ ------------------------- 71.0/212.5 MB 14.2 MB/s eta 0:00:10\n",
      "     ------------ ------------------------- 71.7/212.5 MB 14.2 MB/s eta 0:00:10\n",
      "     ------------ ------------------------- 72.3/212.5 MB 14.2 MB/s eta 0:00:10\n",
      "     ------------- ------------------------ 72.9/212.5 MB 14.2 MB/s eta 0:00:10\n",
      "     ------------- ------------------------ 73.5/212.5 MB 14.2 MB/s eta 0:00:10\n",
      "     ------------- ------------------------ 74.1/212.5 MB 14.2 MB/s eta 0:00:10\n",
      "     ------------- ------------------------ 74.7/212.5 MB 14.2 MB/s eta 0:00:10\n",
      "     ------------- ------------------------ 75.3/212.5 MB 13.9 MB/s eta 0:00:10\n",
      "     ------------- ------------------------ 76.0/212.5 MB 13.9 MB/s eta 0:00:10\n",
      "     ------------- ------------------------ 76.6/212.5 MB 13.9 MB/s eta 0:00:10\n",
      "     ------------- ------------------------ 77.1/212.5 MB 13.6 MB/s eta 0:00:10\n",
      "     ------------- ------------------------ 77.9/212.5 MB 13.4 MB/s eta 0:00:11\n",
      "     -------------- ----------------------- 78.5/212.5 MB 13.4 MB/s eta 0:00:11\n",
      "     -------------- ----------------------- 79.3/212.5 MB 13.4 MB/s eta 0:00:10\n",
      "     -------------- ----------------------- 80.0/212.5 MB 13.1 MB/s eta 0:00:11\n",
      "     -------------- ----------------------- 80.5/212.5 MB 13.1 MB/s eta 0:00:11\n",
      "     -------------- ----------------------- 81.1/212.5 MB 13.1 MB/s eta 0:00:11\n",
      "     -------------- ----------------------- 81.7/212.5 MB 12.8 MB/s eta 0:00:11\n",
      "     -------------- ----------------------- 82.1/212.5 MB 12.6 MB/s eta 0:00:11\n",
      "     -------------- ----------------------- 82.5/212.5 MB 12.4 MB/s eta 0:00:11\n",
      "     -------------- ----------------------- 82.9/212.5 MB 12.1 MB/s eta 0:00:11\n",
      "     -------------- ----------------------- 83.3/212.5 MB 11.9 MB/s eta 0:00:11\n",
      "     -------------- ----------------------- 83.7/212.5 MB 11.7 MB/s eta 0:00:12\n",
      "     --------------- ---------------------- 84.2/212.5 MB 11.7 MB/s eta 0:00:11\n",
      "     --------------- ---------------------- 84.7/212.5 MB 11.5 MB/s eta 0:00:12\n",
      "     --------------- ---------------------- 85.1/212.5 MB 11.3 MB/s eta 0:00:12\n",
      "     --------------- ---------------------- 85.6/212.5 MB 11.3 MB/s eta 0:00:12\n",
      "     --------------- ---------------------- 86.1/212.5 MB 10.9 MB/s eta 0:00:12\n",
      "     --------------- ---------------------- 86.5/212.5 MB 10.9 MB/s eta 0:00:12\n",
      "     --------------- ---------------------- 87.0/212.5 MB 10.9 MB/s eta 0:00:12\n",
      "     --------------- ---------------------- 87.5/212.5 MB 10.7 MB/s eta 0:00:12\n",
      "     --------------- ---------------------- 88.0/212.5 MB 10.7 MB/s eta 0:00:12\n",
      "     --------------- ---------------------- 88.5/212.5 MB 10.6 MB/s eta 0:00:12\n",
      "     --------------- ---------------------- 89.0/212.5 MB 10.6 MB/s eta 0:00:12\n",
      "     ---------------- --------------------- 89.6/212.5 MB 10.4 MB/s eta 0:00:12\n",
      "     ---------------- --------------------- 90.1/212.5 MB 10.4 MB/s eta 0:00:12\n",
      "     ---------------- --------------------- 90.6/212.5 MB 10.4 MB/s eta 0:00:12\n",
      "     ---------------- --------------------- 91.2/212.5 MB 10.2 MB/s eta 0:00:12\n",
      "     ---------------- --------------------- 91.7/212.5 MB 10.2 MB/s eta 0:00:12\n",
      "     ---------------- --------------------- 92.3/212.5 MB 10.4 MB/s eta 0:00:12\n",
      "     ---------------- --------------------- 92.9/212.5 MB 10.7 MB/s eta 0:00:12\n",
      "     ---------------- --------------------- 93.4/212.5 MB 10.7 MB/s eta 0:00:12\n",
      "     ---------------- --------------------- 94.0/212.5 MB 11.1 MB/s eta 0:00:11\n",
      "     ---------------- --------------------- 94.6/212.5 MB 11.1 MB/s eta 0:00:11\n",
      "     ----------------- -------------------- 95.2/212.5 MB 11.3 MB/s eta 0:00:11\n",
      "     ----------------- -------------------- 95.8/212.5 MB 11.5 MB/s eta 0:00:11\n",
      "     ----------------- -------------------- 96.2/212.5 MB 11.5 MB/s eta 0:00:11\n",
      "     ----------------- -------------------- 96.6/212.5 MB 11.5 MB/s eta 0:00:11\n",
      "     ----------------- -------------------- 97.1/212.5 MB 11.3 MB/s eta 0:00:11\n",
      "     ----------------- -------------------- 97.5/212.5 MB 11.3 MB/s eta 0:00:11\n",
      "     ----------------- -------------------- 98.0/212.5 MB 11.3 MB/s eta 0:00:11\n",
      "     ----------------- -------------------- 98.4/212.5 MB 11.1 MB/s eta 0:00:11\n",
      "     ----------------- -------------------- 98.9/212.5 MB 11.1 MB/s eta 0:00:11\n",
      "     ----------------- -------------------- 99.4/212.5 MB 11.1 MB/s eta 0:00:11\n",
      "     ----------------- -------------------- 99.8/212.5 MB 10.9 MB/s eta 0:00:11\n",
      "     ----------------- ------------------- 100.4/212.5 MB 10.9 MB/s eta 0:00:11\n",
      "     ----------------- ------------------- 100.8/212.5 MB 10.9 MB/s eta 0:00:11\n",
      "     ----------------- ------------------- 101.3/212.5 MB 10.9 MB/s eta 0:00:11\n",
      "     ----------------- ------------------- 101.8/212.5 MB 10.9 MB/s eta 0:00:11\n",
      "     ----------------- ------------------- 102.3/212.5 MB 10.7 MB/s eta 0:00:11\n",
      "     ----------------- ------------------- 102.9/212.5 MB 10.9 MB/s eta 0:00:11\n",
      "     ----------------- ------------------- 103.4/212.5 MB 10.7 MB/s eta 0:00:11\n",
      "     ------------------ ------------------ 103.9/212.5 MB 10.7 MB/s eta 0:00:11\n",
      "     ------------------ ------------------ 104.4/212.5 MB 10.7 MB/s eta 0:00:11\n",
      "     ------------------ ------------------ 105.0/212.5 MB 10.7 MB/s eta 0:00:11\n",
      "     ------------------ ------------------ 105.6/212.5 MB 10.7 MB/s eta 0:00:10\n",
      "     ------------------ ------------------ 106.1/212.5 MB 10.7 MB/s eta 0:00:10\n",
      "     ------------------ ------------------ 106.7/212.5 MB 10.9 MB/s eta 0:00:10\n",
      "     ------------------ ------------------ 107.3/212.5 MB 11.1 MB/s eta 0:00:10\n",
      "     ------------------ ------------------ 107.8/212.5 MB 11.1 MB/s eta 0:00:10\n",
      "     ------------------ ------------------ 108.5/212.5 MB 11.3 MB/s eta 0:00:10\n",
      "     ------------------ ------------------ 109.0/212.5 MB 11.5 MB/s eta 0:00:10\n",
      "     ------------------- ----------------- 109.4/212.5 MB 11.3 MB/s eta 0:00:10\n",
      "     ------------------- ----------------- 109.9/212.5 MB 11.3 MB/s eta 0:00:10\n",
      "     ------------------- ----------------- 110.3/212.5 MB 11.3 MB/s eta 0:00:10\n",
      "     ------------------- ----------------- 110.7/212.5 MB 11.3 MB/s eta 0:00:10\n",
      "     ------------------- ----------------- 111.2/212.5 MB 11.1 MB/s eta 0:00:10\n",
      "     ------------------- ----------------- 111.6/212.5 MB 11.1 MB/s eta 0:00:10\n",
      "     ------------------- ----------------- 112.1/212.5 MB 11.1 MB/s eta 0:00:10\n",
      "     ------------------- ----------------- 112.6/212.5 MB 10.9 MB/s eta 0:00:10\n",
      "     ------------------- ----------------- 113.0/212.5 MB 10.9 MB/s eta 0:00:10\n",
      "     ------------------- ----------------- 113.5/212.5 MB 10.9 MB/s eta 0:00:10\n",
      "     ------------------- ----------------- 114.0/212.5 MB 10.9 MB/s eta 0:00:10\n",
      "     ------------------- ----------------- 114.4/212.5 MB 10.7 MB/s eta 0:00:10\n",
      "     ------------------- ----------------- 114.7/212.5 MB 10.4 MB/s eta 0:00:10\n",
      "     -------------------- ---------------- 115.1/212.5 MB 10.2 MB/s eta 0:00:10\n",
      "     -------------------- ---------------- 115.4/212.5 MB 10.1 MB/s eta 0:00:10\n",
      "     -------------------- ----------------- 115.8/212.5 MB 9.9 MB/s eta 0:00:10\n",
      "     -------------------- ----------------- 116.2/212.5 MB 9.8 MB/s eta 0:00:10\n",
      "     -------------------- ----------------- 116.6/212.5 MB 9.6 MB/s eta 0:00:10\n",
      "     -------------------- ----------------- 117.0/212.5 MB 9.6 MB/s eta 0:00:10\n",
      "     -------------------- ----------------- 117.4/212.5 MB 9.5 MB/s eta 0:00:11\n",
      "     --------------------- ---------------- 117.8/212.5 MB 9.4 MB/s eta 0:00:11\n",
      "     --------------------- ---------------- 118.2/212.5 MB 9.2 MB/s eta 0:00:11\n",
      "     --------------------- ---------------- 118.6/212.5 MB 9.1 MB/s eta 0:00:11\n",
      "     --------------------- ---------------- 119.0/212.5 MB 9.0 MB/s eta 0:00:11\n",
      "     --------------------- ---------------- 119.4/212.5 MB 9.0 MB/s eta 0:00:11\n",
      "     --------------------- ---------------- 119.9/212.5 MB 9.0 MB/s eta 0:00:11\n",
      "     --------------------- ---------------- 120.3/212.5 MB 9.0 MB/s eta 0:00:11\n",
      "     --------------------- ---------------- 120.8/212.5 MB 9.0 MB/s eta 0:00:11\n",
      "     --------------------- ---------------- 121.2/212.5 MB 9.0 MB/s eta 0:00:11\n",
      "     --------------------- ---------------- 121.7/212.5 MB 9.0 MB/s eta 0:00:11\n",
      "     --------------------- ---------------- 122.2/212.5 MB 9.0 MB/s eta 0:00:11\n",
      "     --------------------- ---------------- 122.7/212.5 MB 9.0 MB/s eta 0:00:11\n",
      "     ---------------------- --------------- 123.2/212.5 MB 9.0 MB/s eta 0:00:10\n",
      "     ---------------------- --------------- 123.7/212.5 MB 9.1 MB/s eta 0:00:10\n",
      "     ---------------------- --------------- 124.1/212.5 MB 9.1 MB/s eta 0:00:10\n",
      "     ---------------------- --------------- 124.7/212.5 MB 9.2 MB/s eta 0:00:10\n",
      "     ---------------------- --------------- 125.2/212.5 MB 9.5 MB/s eta 0:00:10\n",
      "     ---------------------- --------------- 125.7/212.5 MB 9.8 MB/s eta 0:00:09\n",
      "     ---------------------- --------------- 126.3/212.5 MB 9.9 MB/s eta 0:00:09\n",
      "     ---------------------- -------------- 126.8/212.5 MB 10.1 MB/s eta 0:00:09\n",
      "     ---------------------- -------------- 127.3/212.5 MB 10.2 MB/s eta 0:00:09\n",
      "     ---------------------- -------------- 127.9/212.5 MB 10.4 MB/s eta 0:00:09\n",
      "     ---------------------- -------------- 128.4/212.5 MB 10.6 MB/s eta 0:00:08\n",
      "     ---------------------- -------------- 129.0/212.5 MB 10.7 MB/s eta 0:00:08\n",
      "     ---------------------- -------------- 129.6/212.5 MB 10.9 MB/s eta 0:00:08\n",
      "     ---------------------- -------------- 130.1/212.5 MB 11.1 MB/s eta 0:00:08\n",
      "     ---------------------- -------------- 130.7/212.5 MB 11.3 MB/s eta 0:00:08\n",
      "     ---------------------- -------------- 131.3/212.5 MB 11.5 MB/s eta 0:00:08\n",
      "     ---------------------- -------------- 131.9/212.5 MB 11.7 MB/s eta 0:00:07\n",
      "     ----------------------- ------------- 132.6/212.5 MB 11.7 MB/s eta 0:00:07\n",
      "     ----------------------- ------------- 133.2/212.5 MB 11.9 MB/s eta 0:00:07\n",
      "     ----------------------- ------------- 133.8/212.5 MB 12.1 MB/s eta 0:00:07\n",
      "     ----------------------- ------------- 134.4/212.5 MB 12.4 MB/s eta 0:00:07\n",
      "     ----------------------- ------------- 135.1/212.5 MB 12.4 MB/s eta 0:00:07\n",
      "     ----------------------- ------------- 135.7/212.5 MB 12.6 MB/s eta 0:00:07\n",
      "     ----------------------- ------------- 136.4/212.5 MB 12.8 MB/s eta 0:00:06\n",
      "     ----------------------- ------------- 137.0/212.5 MB 13.1 MB/s eta 0:00:06\n",
      "     ----------------------- ------------- 137.7/212.5 MB 13.1 MB/s eta 0:00:06\n",
      "     ------------------------ ------------ 138.4/212.5 MB 13.4 MB/s eta 0:00:06\n",
      "     ------------------------ ------------ 138.9/212.5 MB 13.6 MB/s eta 0:00:06\n",
      "     ------------------------ ------------ 139.5/212.5 MB 13.4 MB/s eta 0:00:06\n",
      "     ------------------------ ------------ 140.0/212.5 MB 13.1 MB/s eta 0:00:06\n",
      "     ------------------------ ------------ 140.5/212.5 MB 13.1 MB/s eta 0:00:06\n",
      "     ------------------------ ------------ 141.0/212.5 MB 12.8 MB/s eta 0:00:06\n",
      "     ------------------------ ------------ 141.5/212.5 MB 12.8 MB/s eta 0:00:06\n",
      "     ------------------------ ------------ 142.0/212.5 MB 12.6 MB/s eta 0:00:06\n",
      "     ------------------------ ------------ 142.5/212.5 MB 12.6 MB/s eta 0:00:06\n",
      "     ------------------------ ------------ 143.1/212.5 MB 12.3 MB/s eta 0:00:06\n",
      "     ------------------------- ----------- 143.6/212.5 MB 12.4 MB/s eta 0:00:06\n",
      "     ------------------------- ----------- 144.1/212.5 MB 12.4 MB/s eta 0:00:06\n",
      "     ------------------------- ----------- 144.7/212.5 MB 12.1 MB/s eta 0:00:06\n",
      "     ------------------------- ----------- 145.2/212.5 MB 12.1 MB/s eta 0:00:06\n",
      "     ------------------------- ----------- 145.8/212.5 MB 11.9 MB/s eta 0:00:06\n",
      "     ------------------------- ----------- 146.3/212.5 MB 11.9 MB/s eta 0:00:06\n",
      "     ------------------------- ----------- 146.9/212.5 MB 11.7 MB/s eta 0:00:06\n",
      "     ------------------------- ----------- 147.6/212.5 MB 11.9 MB/s eta 0:00:06\n",
      "     ------------------------- ----------- 147.9/212.5 MB 11.5 MB/s eta 0:00:06\n",
      "     ------------------------- ----------- 148.5/212.5 MB 11.5 MB/s eta 0:00:06\n",
      "     ------------------------- ----------- 149.1/212.5 MB 11.3 MB/s eta 0:00:06\n",
      "     -------------------------- ---------- 149.7/212.5 MB 11.5 MB/s eta 0:00:06\n",
      "     -------------------------- ---------- 150.3/212.5 MB 11.5 MB/s eta 0:00:06\n",
      "     -------------------------- ---------- 150.9/212.5 MB 11.7 MB/s eta 0:00:06\n",
      "     -------------------------- ---------- 151.6/212.5 MB 11.9 MB/s eta 0:00:06\n",
      "     -------------------------- ---------- 152.2/212.5 MB 12.1 MB/s eta 0:00:05\n",
      "     -------------------------- ---------- 152.9/212.5 MB 12.1 MB/s eta 0:00:05\n",
      "     -------------------------- ---------- 153.5/212.5 MB 12.6 MB/s eta 0:00:05\n",
      "     -------------------------- ---------- 154.2/212.5 MB 12.8 MB/s eta 0:00:05\n",
      "     -------------------------- ---------- 154.8/212.5 MB 12.9 MB/s eta 0:00:05\n",
      "     --------------------------- --------- 155.3/212.5 MB 12.8 MB/s eta 0:00:05\n",
      "     --------------------------- --------- 155.8/212.5 MB 12.8 MB/s eta 0:00:05\n",
      "     --------------------------- --------- 156.3/212.5 MB 12.6 MB/s eta 0:00:05\n",
      "     --------------------------- --------- 156.8/212.5 MB 12.6 MB/s eta 0:00:05\n",
      "     --------------------------- --------- 157.3/212.5 MB 12.6 MB/s eta 0:00:05\n",
      "     --------------------------- --------- 157.8/212.5 MB 12.4 MB/s eta 0:00:05\n",
      "     --------------------------- --------- 158.3/212.5 MB 12.6 MB/s eta 0:00:05\n",
      "     --------------------------- --------- 158.8/212.5 MB 12.4 MB/s eta 0:00:05\n",
      "     --------------------------- --------- 159.3/212.5 MB 12.4 MB/s eta 0:00:05\n",
      "     --------------------------- --------- 159.9/212.5 MB 12.1 MB/s eta 0:00:05\n",
      "     --------------------------- --------- 160.3/212.5 MB 11.9 MB/s eta 0:00:05\n",
      "     ---------------------------- -------- 160.8/212.5 MB 11.9 MB/s eta 0:00:05\n",
      "     ---------------------------- -------- 161.4/212.5 MB 11.9 MB/s eta 0:00:05\n",
      "     ---------------------------- -------- 161.9/212.5 MB 11.7 MB/s eta 0:00:05\n",
      "     ---------------------------- -------- 162.5/212.5 MB 11.7 MB/s eta 0:00:05\n",
      "     ---------------------------- -------- 163.1/212.5 MB 11.7 MB/s eta 0:00:05\n",
      "     ---------------------------- -------- 163.6/212.5 MB 11.5 MB/s eta 0:00:05\n",
      "     ---------------------------- -------- 164.2/212.5 MB 11.5 MB/s eta 0:00:05\n",
      "     ---------------------------- -------- 164.8/212.5 MB 11.3 MB/s eta 0:00:05\n",
      "     ---------------------------- -------- 165.4/212.5 MB 11.5 MB/s eta 0:00:05\n",
      "     ---------------------------- -------- 166.1/212.5 MB 11.7 MB/s eta 0:00:04\n",
      "     ----------------------------- ------- 166.7/212.5 MB 11.7 MB/s eta 0:00:04\n",
      "     ----------------------------- ------- 167.3/212.5 MB 11.9 MB/s eta 0:00:04\n",
      "     ----------------------------- ------- 167.9/212.5 MB 12.1 MB/s eta 0:00:04\n",
      "     ----------------------------- ------- 168.5/212.5 MB 12.4 MB/s eta 0:00:04\n",
      "     ----------------------------- ------- 169.2/212.5 MB 12.4 MB/s eta 0:00:04\n",
      "     ----------------------------- ------- 169.9/212.5 MB 12.6 MB/s eta 0:00:04\n",
      "     ----------------------------- ------- 170.5/212.5 MB 12.8 MB/s eta 0:00:04\n",
      "     ----------------------------- ------- 171.2/212.5 MB 13.1 MB/s eta 0:00:04\n",
      "     ----------------------------- ------- 171.7/212.5 MB 13.1 MB/s eta 0:00:04\n",
      "     ------------------------------ ------ 172.4/212.5 MB 13.1 MB/s eta 0:00:04\n",
      "     ------------------------------ ------ 172.8/212.5 MB 13.1 MB/s eta 0:00:04\n",
      "     ------------------------------ ------ 173.3/212.5 MB 12.8 MB/s eta 0:00:04\n",
      "     ------------------------------ ------ 173.8/212.5 MB 12.8 MB/s eta 0:00:04\n",
      "     ------------------------------ ------ 174.3/212.5 MB 12.6 MB/s eta 0:00:04\n",
      "     ------------------------------ ------ 174.8/212.5 MB 12.6 MB/s eta 0:00:03\n",
      "     ------------------------------ ------ 175.3/212.5 MB 12.4 MB/s eta 0:00:04\n",
      "     ------------------------------ ------ 175.9/212.5 MB 12.4 MB/s eta 0:00:03\n",
      "     ------------------------------ ------ 176.4/212.5 MB 12.1 MB/s eta 0:00:03\n",
      "     ------------------------------ ------ 176.9/212.5 MB 12.1 MB/s eta 0:00:03\n",
      "     ------------------------------ ------ 177.4/212.5 MB 11.9 MB/s eta 0:00:03\n",
      "     ------------------------------ ------ 178.0/212.5 MB 11.9 MB/s eta 0:00:03\n",
      "     ------------------------------- ----- 178.5/212.5 MB 11.9 MB/s eta 0:00:03\n",
      "     ------------------------------- ----- 179.1/212.5 MB 11.7 MB/s eta 0:00:03\n",
      "     ------------------------------- ----- 179.6/212.5 MB 11.7 MB/s eta 0:00:03\n",
      "     ------------------------------- ----- 180.2/212.5 MB 11.5 MB/s eta 0:00:03\n",
      "     ------------------------------- ----- 180.7/212.5 MB 11.7 MB/s eta 0:00:03\n",
      "     ------------------------------- ----- 181.2/212.5 MB 11.3 MB/s eta 0:00:03\n",
      "     ------------------------------- ----- 181.6/212.5 MB 11.1 MB/s eta 0:00:03\n",
      "     ------------------------------- ----- 182.0/212.5 MB 11.1 MB/s eta 0:00:03\n",
      "     ------------------------------- ----- 182.4/212.5 MB 10.9 MB/s eta 0:00:03\n",
      "     ------------------------------- ----- 182.9/212.5 MB 10.9 MB/s eta 0:00:03\n",
      "     ------------------------------- ----- 183.3/212.5 MB 10.6 MB/s eta 0:00:03\n",
      "     ------------------------------- ----- 183.8/212.5 MB 10.7 MB/s eta 0:00:03\n",
      "     -------------------------------- ---- 184.2/212.5 MB 10.7 MB/s eta 0:00:03\n",
      "     -------------------------------- ---- 184.7/212.5 MB 10.7 MB/s eta 0:00:03\n",
      "     -------------------------------- ---- 185.1/212.5 MB 10.6 MB/s eta 0:00:03\n",
      "     -------------------------------- ---- 185.6/212.5 MB 10.6 MB/s eta 0:00:03\n",
      "     -------------------------------- ---- 186.1/212.5 MB 10.6 MB/s eta 0:00:03\n",
      "     -------------------------------- ---- 186.6/212.5 MB 10.6 MB/s eta 0:00:03\n",
      "     -------------------------------- ---- 187.1/212.5 MB 10.6 MB/s eta 0:00:03\n",
      "     -------------------------------- ---- 187.6/212.5 MB 10.4 MB/s eta 0:00:03\n",
      "     -------------------------------- ---- 188.0/212.5 MB 10.4 MB/s eta 0:00:03\n",
      "     -------------------------------- ---- 188.6/212.5 MB 10.4 MB/s eta 0:00:03\n",
      "     -------------------------------- ---- 189.1/212.5 MB 10.4 MB/s eta 0:00:03\n",
      "     --------------------------------- --- 189.7/212.5 MB 10.4 MB/s eta 0:00:03\n",
      "     --------------------------------- --- 190.2/212.5 MB 10.4 MB/s eta 0:00:03\n",
      "     --------------------------------- --- 190.7/212.5 MB 10.2 MB/s eta 0:00:03\n",
      "     --------------------------------- --- 191.3/212.5 MB 10.4 MB/s eta 0:00:03\n",
      "     --------------------------------- --- 191.8/212.5 MB 10.6 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 192.4/212.5 MB 10.7 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 193.0/212.5 MB 10.9 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 193.5/212.5 MB 10.9 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 194.2/212.5 MB 11.1 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 194.8/212.5 MB 11.3 MB/s eta 0:00:02\n",
      "     ---------------------------------- -- 195.3/212.5 MB 11.5 MB/s eta 0:00:02\n",
      "     ---------------------------------- -- 196.0/212.5 MB 11.7 MB/s eta 0:00:02\n",
      "     ---------------------------------- -- 196.6/212.5 MB 11.9 MB/s eta 0:00:02\n",
      "     ---------------------------------- -- 197.2/212.5 MB 12.1 MB/s eta 0:00:02\n",
      "     ---------------------------------- -- 197.8/212.5 MB 12.1 MB/s eta 0:00:02\n",
      "     ---------------------------------- -- 198.5/212.5 MB 12.4 MB/s eta 0:00:02\n",
      "     ---------------------------------- -- 199.1/212.5 MB 12.6 MB/s eta 0:00:02\n",
      "     ---------------------------------- -- 199.7/212.5 MB 12.8 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 200.4/212.5 MB 12.8 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 201.1/212.5 MB 13.1 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 201.7/212.5 MB 13.1 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 202.1/212.5 MB 13.1 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 202.4/212.5 MB 12.6 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 202.8/212.5 MB 12.6 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 203.1/212.5 MB 12.1 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 203.5/212.5 MB 11.9 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 203.8/212.5 MB 11.7 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 204.2/212.5 MB 11.5 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 204.6/212.5 MB 11.3 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 204.9/212.5 MB 11.1 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 205.3/212.5 MB 10.9 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 205.8/212.5 MB 10.7 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 206.1/212.5 MB 10.6 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 206.6/212.5 MB 10.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  207.0/212.5 MB 10.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  207.4/212.5 MB 10.1 MB/s eta 0:00:01\n",
      "     -------------------------------------  207.8/212.5 MB 9.9 MB/s eta 0:00:01\n",
      "     -------------------------------------  208.3/212.5 MB 9.8 MB/s eta 0:00:01\n",
      "     -------------------------------------  208.7/212.5 MB 9.6 MB/s eta 0:00:01\n",
      "     -------------------------------------  209.2/212.5 MB 9.5 MB/s eta 0:00:01\n",
      "     -------------------------------------  209.6/212.5 MB 9.4 MB/s eta 0:00:01\n",
      "     -------------------------------------  210.1/212.5 MB 9.2 MB/s eta 0:00:01\n",
      "     -------------------------------------  210.6/212.5 MB 9.1 MB/s eta 0:00:01\n",
      "     -------------------------------------  211.1/212.5 MB 9.1 MB/s eta 0:00:01\n",
      "     -------------------------------------  211.6/212.5 MB 9.0 MB/s eta 0:00:01\n",
      "     -------------------------------------  212.0/212.5 MB 9.0 MB/s eta 0:00:01\n",
      "     -------------------------------------  212.5/212.5 MB 9.0 MB/s eta 0:00:01\n",
      "     -------------------------------------  212.5/212.5 MB 9.0 MB/s eta 0:00:01\n",
      "     -------------------------------------  212.5/212.5 MB 9.0 MB/s eta 0:00:01\n",
      "     -------------------------------------  212.5/212.5 MB 9.0 MB/s eta 0:00:01\n",
      "     -------------------------------------  212.5/212.5 MB 9.0 MB/s eta 0:00:01\n",
      "     -------------------------------------  212.5/212.5 MB 9.0 MB/s eta 0:00:01\n",
      "     -------------------------------------  212.5/212.5 MB 9.0 MB/s eta 0:00:01\n",
      "     -------------------------------------  212.5/212.5 MB 9.0 MB/s eta 0:00:01\n",
      "     -------------------------------------- 212.5/212.5 MB 5.9 MB/s eta 0:00:00\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.22.0-cp310-cp310-win_amd64.whl (1.7 MB)\n",
      "     ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "     ------------ --------------------------- 0.6/1.7 MB 11.5 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 1.1/1.7 MB 13.9 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 1.6/1.7 MB 11.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.7/1.7 MB 10.9 MB/s eta 0:00:00\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.7.0-cp310-cp310-win_amd64.whl (2.5 MB)\n",
      "     ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "     -------- ------------------------------- 0.5/2.5 MB 16.5 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 1.1/2.5 MB 11.3 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 1.6/2.5 MB 11.5 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 2.2/2.5 MB 11.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.5/2.5 MB 11.3 MB/s eta 0:00:00\n",
      "Collecting networkx\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "     ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "     ----------- ---------------------------- 0.5/1.7 MB 16.2 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 1.1/1.7 MB 13.9 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 1.7/1.7 MB 13.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.7/1.7 MB 11.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kemal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch) (3.1.6)\n",
      "Collecting sympy>=1.13.3\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "     ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 0.6/6.3 MB 17.5 MB/s eta 0:00:01\n",
      "     ------- -------------------------------- 1.1/6.3 MB 14.3 MB/s eta 0:00:01\n",
      "     ---------- ----------------------------- 1.7/6.3 MB 13.8 MB/s eta 0:00:01\n",
      "     -------------- ------------------------- 2.4/6.3 MB 13.6 MB/s eta 0:00:01\n",
      "     ------------------ --------------------- 3.0/6.3 MB 13.5 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 3.6/6.3 MB 13.4 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 4.2/6.3 MB 13.4 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 4.8/6.3 MB 13.4 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 5.5/6.3 MB 13.5 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 6.1/6.3 MB 13.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 6.3/6.3 MB 13.0 MB/s eta 0:00:00\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\kemal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch) (4.13.2)\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2025.5.0-py3-none-any.whl (196 kB)\n",
      "     ---------------------------------------- 0.0/196.2 kB ? eta -:--:--\n",
      "     ------------------------------------  194.6/196.2 kB 12.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- 196.2/196.2 kB 4.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in c:\\users\\kemal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torchvision) (2.2.6)\n",
      "Collecting pillow!=8.3.*,>=5.3.0\n",
      "  Downloading pillow-11.2.1-cp310-cp310-win_amd64.whl (2.7 MB)\n",
      "     ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "     ----- ---------------------------------- 0.4/2.7 MB 8.5 MB/s eta 0:00:01\n",
      "     ------------- -------------------------- 0.9/2.7 MB 9.2 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 1.3/2.7 MB 10.6 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 1.8/2.7 MB 10.5 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 2.3/2.7 MB 10.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.7/2.7 MB 10.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.7/2.7 MB 9.5 MB/s eta 0:00:00\n",
      "Collecting mpmath<1.4,>=1.1.0\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "     ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "     ---------------------------- -------- 419.8/536.2 kB 13.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- 536.2/536.2 kB 8.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kemal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Installing collected packages: mpmath, sympy, pillow, networkx, fsspec, filelock, torch, torchvision, torchaudio\n",
      "Successfully installed filelock-3.18.0 fsspec-2025.5.0 mpmath-1.3.0 networkx-3.4.2 pillow-11.2.1 sympy-1.14.0 torch-2.7.0 torchaudio-2.7.0 torchvision-0.22.0\n",
      "2.7.0+cpu\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "print(torch.__version__) # Just to check the version and that torch is loaded\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "\n",
    "def model(w: Tensor, x: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Takes the coefficients w, the observations, computes the polynomial and\n",
    "    returns probabilities.\n",
    "    w: Tensor of shape (..., D+1) containing [w0, w1, ..., wD]\n",
    "    x: Tensor of shape (N, D) containing D-dimensional inputs for N samples\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (N,) with predicted probabilities in [0,1]\n",
    "    \"\"\"\n",
    "    # Number of features\n",
    "    D = x.shape[1]\n",
    "    # Ensure w has D+1 coefficients\n",
    "    assert w.shape[-1] == D + 1, f\"Expected w[..., D+1], got {w.shape[-1]}\"\n",
    "\n",
    "    # Compute the polynomial z = w0 + w1*x1 + w2*(x2^2) + ... + wD*(xD^D)\n",
    "    # Broadcast w terms against x dimensions\n",
    "    z = w[..., 0]  # bias term, shape broadcastable to (N,)\n",
    "    for j in range(1, D + 1):\n",
    "        # x[:, j-1] raised to the j-th power, shape (N,)\n",
    "        term = x[:, j-1] ** j\n",
    "        # w[..., j] has shape like (...,)\n",
    "        z = z + w[..., j] * term\n",
    "    # Apply sigmoid to get probabilities\n",
    "    return torch.sigmoid(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a06c5b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6672, 0.7226, 0.7351, 0.7392])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity-check, we should get 4 outputs if the model is correctly vectorized!\n",
    "temp = model(w=torch.rand(size=(1,6)), x=torch.rand(size=(4,5)))\n",
    "temp\n",
    "\n",
    "# output that was provided (double check):\n",
    "#tensor([0.7839, 0.7999, 0.7693, 0.8770])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cd167f",
   "metadata": {},
   "source": [
    "## The Likelihood function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c3d50b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_lik(y_true: Tensor, y_hat: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    For one or more observations, where y_true is the true label (0 or 1)\n",
    "    and y_hat contains predicted probabilities [0,1], computes the (log)\n",
    "    likelihood summed over all observations.\n",
    "\n",
    "    Returns a single-element tensor.\n",
    "    \"\"\"\n",
    "    # Avoid log(0) by clamping y_hat within (eps, 1-eps)\n",
    "    eps = 1e-8\n",
    "    p = y_hat.clamp(min=eps, max=1.0 - eps)\n",
    "    # Log-likelihood: sum_i [y_i*log(p_i) + (1-y_i)*log(1-p_i)]\n",
    "    ll = (y_true * torch.log(p) + (1 - y_true) * torch.log(1 - p)).sum()\n",
    "    return ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f5f9ac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-3.3387)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity-check, should be a single element here:\n",
    "log_lik(y_true=torch.tensor(data=[1,0,1,0], dtype=torch.float), y_hat=temp)\n",
    "\n",
    "# output that was provided (double check):\n",
    "# tensor(-3.9837)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28d602e",
   "metadata": {},
   "source": [
    "# The Evidence Lower BOund (ELBO)\n",
    "\n",
    "Optimizing the ELBO (maximization) is the same as minimizing the KL divergence.\n",
    "Here, the students shall implement a **Monte Carlo** approximation.\n",
    "\n",
    "According to the slides, this is how it's done:\n",
    "\n",
    "1. Sample from approximate posterior distribution $q_{\\phi}(\\theta)$.\n",
    "    * Direct sampling methods should be used (we have a well-defined mean-field approximation here).\n",
    "    * Apply reparameterization trick to train the model. Note: Do **not** use amortized variational inference here.\n",
    "2. Estimate the ELBO using stochastic gradient-based optimization.\n",
    "\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911c504c",
   "metadata": {},
   "source": [
    "Remember that the Bayesian framework tells us something about the *model*.\n",
    "The variational approach allows us to **empirically** estimate the overall goodness of fit of our model.\n",
    "\n",
    "In order to do that sufficiently well, we need more than just point estimates.\n",
    "Recall that we do **not** attempt to find some best point estimates for our data, but rather a distribution over them.\n",
    "In order for that to work well, we need to test many different parameter constellations and average over those results.\n",
    "In other words, we need to draw many different sets of possible variational distribution parameters and check how well these allow our model, on average, to predict the constant observations.\n",
    "\n",
    "\n",
    "-------\n",
    "\n",
    "Recall the definition of the ELBO:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\text{ELBO}&=\\mathbb{E}_{q_{\\phi}(\\mathbf{w})}\\left[\\log{(p(\\mathbf{y}|\\mathbf{X},\\mathbf{w}))}\\right]-D_{\\text{KL}}\\left(q_{\\phi}(\\mathbf{w})\\|p(\\mathbf{w})\\right).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "-------\n",
    "\n",
    "For simplicity here, assume there will be the following \"loops\":\n",
    "\n",
    "1. Outermost loop is governed by the epoch ($E$).\n",
    "2. The next loop is over the stochastic mini-batches of observations (batches of size $N$).\n",
    "    * For each batch, you draw $w^{(s)}\\in 1\\dots S$ **new** different sets of parameter configurations from your variational distribution.\n",
    "    * Here, you'll be using the reparameterization trick.\n",
    "3. Loop over $S$:\n",
    "    * (a) For each configuration $s_i$, you will have to multiply (sum) the (log) likelihood of each observation under the current likelihood function (as parameterized by $w^{(s)}$).\n",
    "    * (b) Next, calculate the KL-divergence between our approximate posterior (the mean-field approximation of independent Gaussians) and our prior (which is a diagonal standard normal distribution). **Attention**:\n",
    "        * You use either, the **analytical** or the **Monte Carlo** approximation of the KL-divergence. However, the analytical one is essentially *outside the expectation* (because it does not average over $w^{(s)}$), whereas the MC-approximation should perhaps be an addend/subtrahend to (a).\n",
    "        * In effect, the analytical KL-divergence is calculated only **once** per batch, using the **current** variational parameters (i.e., as they were after the last optimizer's step or after initialization for first step).\n",
    "    * Calculate (a) - (b), according to previous remark.\n",
    "4. Average the results from step 3 (considering the remark about analytical/MC version of the KL-divergence). It needs to be an average because the ELBO is an expectation (a weighted mean) over all possible realizations of $s_i\\in S$. For each individual batch, you have now an average idea of:\n",
    "    * The *expected* data likelihood.\n",
    "    * How strongly your prior and approximate posterior diverge from one another.\n",
    "5. Do not accumulate results across batches at this point.\n",
    "    * It is better to compute a gradient for each batch and apply parameter updates. Frequent, incremental updates work better in practice.\n",
    "\n",
    "\n",
    "**Notes**:\n",
    "* As $S$ approaches $\\infty$, the MC-approximation of the KL-divergence will be equal to the analytical solution.\n",
    "* For the MC-approximation, a good $S$ is perhaps $50-500$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3a1640",
   "metadata": {},
   "source": [
    "# Putting it all together\n",
    "\n",
    "Let's implement the \"loops\" from above.\n",
    "We will create stochastic mini-batches of our data to compute gradients on.\n",
    "For now, we will not implement a loop for epochs.\n",
    "\n",
    "The ELBO for a list of parameters sets $S$ and a mini-batch of length $N$ is defined as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\text{ELBO}(\\phi)&\\approx\\left(\\frac{1}{S}\\sum_{s=1}^{S}\\,\\underbrace{\\left[\\sum_{i=1}^{N}\\,\\log{(p(y_i|x_i,w^{(s)}))}\\right]}_{\\text{log-sum of i.i.d. observations}}\\right)-\\underbrace{D_{\\text{KL}}(q_{\\mu,\\sigma}(w)\\|p(w))}_{\\text{using the current $\\mu,\\sigma$}}.\n",
    "    \\\\[4em]\n",
    "    \\text{Also note that}&\\text{ we can use Monte Carlo to approximate the KL-divergence:}\\nonumber\n",
    "    \\\\[1em]\n",
    "    D_{\\text{KL}}(q_{\\mu,\\sigma}(w)\\|p(w))&=\\mathbb{E}_{w\\sim q_{\\mu,\\sigma}(w)}\\left[\\log{\\left(\\frac{q_{\\mu,\\sigma}(w)}{p(w)}\\right)}\\right],\n",
    "    \\\\[1em]\n",
    "    &=\\frac{1}{S}\\sum_{s=1}^{S}\\,\\left[\\log{\\left(q_{\\mu,\\sigma}\\left(w^{(s)}\\right)\\right)}-\\log{\\left(p\\left(w^{(s)}\\right)\\right)}\\right].\n",
    "    \\\\[2em]\n",
    "    \\text{Also note that}&\\text{ the KL-divergence between two normals has an analytical solution:}\\nonumber\n",
    "    \\\\[1em]\n",
    "    D_{\\text{KL}}(q_{\\mu,\\sigma}(w)\\|\\mathcal{N}(0,I))&=\\frac{1}{2}\\sum_{j=0}\\,\\left(\\mu_j^2+\\sigma_j^2-1-\\log{\\left(\\sigma_j^2\\right)}\\right).\n",
    "    \\\\[2em]\n",
    "    \\text{Also note that}&\\text{ the ELBO including the MC-approximation of the KL-divergence is:}\\nonumber\n",
    "    \\\\[1em]\n",
    "    \\text{ELBO}(\\phi)&\\approx\\frac{1}{S}\\sum_{s=1}^{S}\\,\\underbrace{\\left[\\sum_{i=1}^{N}\\,\\log{(p(y_i|x_i,w^{(s)}))}\\right]}_{\\text{log-sum of i.i.d. observations}}-\\underbrace{\\left[\\log{\\left(q_{\\mu,\\sigma}\\left(w^{(s)}\\right)\\right)}-\\log{\\left(p\\left(w^{(s)}\\right)\\right)}\\right]}_{\\text{MC-approx. of KL-divergence}}.\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc32b04f",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "\n",
    "Here, implement everything you need.\n",
    "\n",
    "## Data Preparation and Dimensionality Reduction\n",
    "\n",
    "Start by reducing the dimensionality of your data.\n",
    "Choose a combination of dataset and polynomial degree that is not too low and not too high.\n",
    "Aim for at least 30% explained variance and at most 15 components.\n",
    "\n",
    "Report the (sum of the) explained variance before you proceed.\n",
    "\n",
    "\n",
    "Remember the basics: splitting, randomness, scaling, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9de2e774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 3 PCA components capturing 0.342 variance\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Literal\n",
    "\n",
    "data = np.load(file='2048d_sms_spam_albert-xlarge-v2.npz')\n",
    "# data = np.load(file='300d_sms_spam_fasttext_pca.npz')\n",
    "\n",
    "embeddings = data['X']\n",
    "labels = data['y']\n",
    "\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    embeddings, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test  = scaler.transform(X_test)\n",
    "\n",
    "# PCA: at least 30% variance, at most 15 components\n",
    "pca = PCA(n_components=min(15, X_train.shape[1]))\n",
    "pca.fit(X_train)\n",
    "cumvar = np.cumsum(pca.explained_variance_ratio_)\n",
    "n_components = int(np.searchsorted(cumvar, 0.30) + 1)\n",
    "print(f\"Using {n_components} PCA components capturing {cumvar[n_components-1]:.3f} variance\")\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test  = pca.transform(X_test)\n",
    "\n",
    "scaler2 = StandardScaler() # scale and standardize again after PCA\n",
    "X_train = scaler2.fit_transform(X_train)\n",
    "X_test = scaler2.transform(X_test)\n",
    "\n",
    "\n",
    "# Convert to torch tensors\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_t  = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_t  = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Mini-batch loader\n",
    "BATCH_SIZE = 64\n",
    "train_ds = TensorDataset(X_train_t, y_train_t)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# --- MODEL AND HELPER FUNCTIONS ---\n",
    "\n",
    "POLY_DEGREE = n_components  # use PCA components as degree\n",
    "\n",
    "def model(w: Tensor, x: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Polynomial logistic model: z = w0 + _{j=1}^D w_j * x_j^j, then sigmoid.\n",
    "    - w: (..., D+1)\n",
    "    - x: (N, D)\n",
    "    Returns: Tensor of shape (N,) probabilities.\n",
    "    \"\"\"\n",
    "    D = x.shape[1]\n",
    "    assert w.shape[-1] == D + 1\n",
    "    z = w[..., 0]\n",
    "    for j in range(1, D+1):\n",
    "        # More aggressive scaling of polynomial terms\n",
    "        z = z + w[..., j] * (x[:, j-1] ** j) / (j * 100)\n",
    "    return torch.sigmoid(z)\n",
    "\n",
    "\n",
    "def log_lik(y_true: Tensor, y_hat: Tensor) -> Tensor:\n",
    "    \"\"\"Sum log-likelihood for Bernoulli outcomes.\"\"\"\n",
    "    eps = 1e-8\n",
    "    p = y_hat.clamp(eps, 1 - eps)\n",
    "    # Use log-sum-exp trick for numerical stability\n",
    "    log_p = torch.log(p)\n",
    "    log_1_p = torch.log(1 - p)\n",
    "    # Add small epsilon to prevent -inf\n",
    "    return (y_true * log_p + (1-y_true) * log_1_p + eps).sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17a1521",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Below you'll find some prototypes (fill in the blanks).\n",
    "Again, this is just a suggestion, you can come up with your own implementation.\n",
    "\n",
    "Here i had a really annoying issue where i would keep getting NaN values for my ELBO during training. I have commented out some print statements that helped me pinpoint the cuase of this but i will keep them here becuase if i want to experiment further with this in my free-time i would like to have this here. My conclusion for the weird bug is that the log function made it so that we would get extremely large decimal points for negative values. So we would get something like 0.000000...0001 and when it hit 0 it the log function would return -inf and when theese -inf sums get summed up, python will break and produce NaN and this will poison all future gradients.\n",
    "\n",
    "So what i did is that i \"clamp\" the arguments to log so they never hit 0 and this makes sure that i dont get a division by zero in the KL and the NaNs dissappear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1cbe05",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_VARIATIONAL_SETS = 50\n",
    "\n",
    "def ELBO_expected_data_likelihood(y_true: Tensor, x: Tensor, W: Tensor) -> Tensor:\n",
    "    \"\"\"A function to calculate the first term of the ELBO.    Monte Carlo estimate of E_{q}[log p(y|x,w)].\n",
    "    - W: shape (S, D+1)\n",
    "    \"\"\"\n",
    "    device = y_true.device\n",
    "    ll_sum = torch.zeros((), device=device)\n",
    "    S = W.shape[0]\n",
    "    for s in range(S):\n",
    "        w_s = W[s]\n",
    "        y_hat = model(w_s, x)\n",
    "        # Add small epsilon to prevent log(0)\n",
    "        y_hat = y_hat.clamp(min=1e-8, max=1-1e-8)\n",
    "        ll = log_lik(y_true, y_hat)\n",
    "        \n",
    "        # Debug prints for first sample\n",
    "        #if s == 0:\n",
    "            #print(f\"y_hat range: [{y_hat.min():.4f}, {y_hat.max():.4f}]\")\n",
    "            #print(f\"log_lik: {ll:.4f}\")\n",
    "        \n",
    "        ll_sum = ll_sum + ll\n",
    "    \n",
    "    return ll_sum / float(S)\n",
    "\n",
    "\n",
    "def ELBO_KL_divergence_analytical(mu: Tensor, sigma: Tensor) -> Tensor:\n",
    "    \"\"\"The analytical version of the KL divergence.    KL[q||p] for q = N(mu, diag(sigma^2)), p = N(0,I).\n",
    "    \"\"\"\n",
    "    # Add small epsilon to prevent log(0) and ensure numerical stability\n",
    "    sigma_sq = sigma**2 + 1e-6\n",
    "    # Clip mu to prevent extreme values\n",
    "    mu_clipped = torch.clamp(mu, min=-10.0, max=10.0)\n",
    "    # Ensure sigma_sq is not too small or too large\n",
    "    sigma_sq = torch.clamp(sigma_sq, min=1e-6, max=1e6)\n",
    "    \n",
    "    kl = 0.5 * torch.sum(mu_clipped**2 + sigma_sq - 1 - torch.log(sigma_sq))\n",
    "    \n",
    "    # Debug prints\n",
    "    #print(\"Debug ELBO_KL_divergence_analytical:\")\n",
    "    #print(f\"mu range: [{mu.min():.4f}, {mu.max():.4f}]\")\n",
    "    #print(f\"sigma range: [{sigma.min():.4f}, {sigma.max():.4f}]\")\n",
    "    #print(f\"sigma_sq range: [{sigma_sq.min():.4f}, {sigma_sq.max():.4f}]\")\n",
    "    #print(f\"kl: {kl:.4f}\")\n",
    "    \n",
    "    return kl\n",
    "\n",
    "\n",
    "def ELBO_KL_divergence_Monte_Carlo(W: Tensor, mu: Tensor, sigma: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    MC estimate of KL divergence via samples W ~ q.\n",
    "    W shape: (S, D+1)\n",
    "    \"\"\"\n",
    "    S, Dp = W.shape\n",
    "    # Add small epsilon to prevent division by zero\n",
    "    sigma_safe = sigma + 1e-8\n",
    "    \n",
    "    # Debug prints if NaN\n",
    "    #if torch.isnan(W).any() or torch.isnan(mu).any() or torch.isnan(sigma).any():\n",
    "        #print(\"Debug ELBO_KL_divergence_Monte_Carlo:\")\n",
    "        #print(f\"W range: [{W.min():.4f}, {W.max():.4f}]\")\n",
    "        #print(f\"mu range: [{mu.min():.4f}, {mu.max():.4f}]\")\n",
    "        #print(f\"sigma range: [{sigma.min():.4f}, {sigma.max():.4f}]\")\n",
    "    \n",
    "    # log q and log p\n",
    "    log_q = -0.5 * (((W - mu) / sigma_safe) ** 2 + torch.log(2 * torch.pi * sigma_safe**2))\n",
    "    log_p = -0.5 * (W**2 + torch.log(2 * torch.pi))\n",
    "    \n",
    "    # Debug prints if NaN\n",
    "    #if torch.isnan(log_q).any() or torch.isnan(log_p).any():\n",
    "        #print(f\"log_q range: [{log_q.min():.4f}, {log_q.max():.4f}]\")\n",
    "        #print(f\"log_p range: [{log_p.min():.4f}, {log_p.max():.4f}]\")\n",
    "    \n",
    "    # sum over dims, mean over samples\n",
    "    return torch.mean(torch.sum(log_q - log_p, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5d5e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_VARIATIONAL_SETS = 50\n",
    "KL_DIV_TYPE = Literal['analytical', 'montecarlo']\n",
    "\n",
    "\n",
    "def ELBO(use_mu: Tensor, use_sigma: Tensor, y_true: Tensor, obs: Tensor, variational_params_noise: Tensor, kl: KL_DIV_TYPE='analytical', return_exp_data_lik: bool=False, return_kl_div: bool=False) -> Tensor|tuple[Tensor, ...]:\n",
    "    \n",
    "    # Reparameterization: W = mu + sigma * eps\n",
    "    W = use_mu.unsqueeze(0) + use_sigma.unsqueeze(0) * variational_params_noise   # (S, D+1)\n",
    "    exp_lik = ELBO_expected_data_likelihood(y_true, obs, W)\n",
    "    if kl == 'analytical':\n",
    "        kl_div = ELBO_KL_divergence_analytical(use_mu, use_sigma)\n",
    "    else:\n",
    "        kl_div = ELBO_KL_divergence_Monte_Carlo(W, use_mu, use_sigma)\n",
    "    elbo = exp_lik - kl_div\n",
    "    outputs = (elbo,)\n",
    "    if return_exp_data_lik: outputs += (exp_lik,)\n",
    "    if return_kl_div:      outputs += (kl_div,)\n",
    "    return outputs[0] if len(outputs)==1 else outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d6ea3a",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Train your model until convergence.\n",
    "Choose a number of iterations, batch-size, and learning rate that make sense in your scenario.\n",
    "Do not perform a grid search or other hyperparameter optimization.\n",
    "Instead, manually find some good working parameters and make your final solution just use these.\n",
    "\n",
    "\n",
    "* **Plot** the training curve (i.e., plot the history for each component of the ELBO).\n",
    "* **Print** the final parameters for your variational distribution after optimization.\n",
    "* **Evaluate** the ELBO on the holdout dataset. Is it close? You could also do this during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f128226f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 ELBO: -49.1471\n",
      "Epoch 10 ELBO: -35.4965\n",
      "Epoch 20 ELBO: -29.8945\n",
      "Epoch 30 ELBO: -28.2986\n",
      "Epoch 40 ELBO: -27.8282\n",
      "Final mu: tensor([-1.6487,  0.0511,  0.1097, -0.0420])\n",
      "Final sigma: tensor([0.3292, 0.9991, 1.0003, 0.9970])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set ELBO: -450.1004\n"
     ]
    }
   ],
   "source": [
    "D_plus1 = X_train_t.shape[1] + 1\n",
    "mu_param = torch.zeros(D_plus1, requires_grad=True)\n",
    "log_sigma_param = torch.zeros(D_plus1, requires_grad=True)\n",
    "\n",
    "optimizer = torch.optim.Adam([mu_param, log_sigma_param], lr=1e-3)\n",
    "\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 0.0001\n",
    "USE_KL_TYPE: KL_DIV_TYPE = 'analytical'\n",
    "elbo_history = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_elbo = 0.0\n",
    "    for Xb, yb in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        sigma = torch.exp(log_sigma_param)\n",
    "        eps = torch.randn(NUM_VARIATIONAL_SETS, D_plus1)\n",
    "        elbo = ELBO(mu_param, sigma, yb, Xb, eps, kl=USE_KL_TYPE)\n",
    "        loss = -elbo\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_elbo += elbo.item()\n",
    "    elbo_history.append(epoch_elbo / len(train_loader))\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch:2d} ELBO: {elbo_history[-1]:.4f}\")\n",
    "\n",
    "# Final variational parameters\n",
    "final_mu = mu_param.detach()\n",
    "final_sigma = torch.exp(log_sigma_param).detach()\n",
    "print(\"Final mu:\", final_mu)\n",
    "print(\"Final sigma:\", final_sigma)\n",
    "\n",
    "# Plot training ELBO\n",
    "plt.plot(elbo_history)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('ELBO')\n",
    "plt.title('ELBO over Training')\n",
    "plt.show()\n",
    "\n",
    "# ELBO on test data\n",
    "eps_test = torch.randn(NUM_VARIATIONAL_SETS, D_plus1)\n",
    "test_elbo = ELBO(mu_param, torch.exp(log_sigma_param), y_test_t, X_test_t, eps_test, kl=USE_KL_TYPE)\n",
    "print(f\"Test set ELBO: {test_elbo:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91e17048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set ELBO per sample: -0.4037\n"
     ]
    }
   ],
   "source": [
    "test_elbo_per_sample = (test_elbo.item() / len(y_test_t))\n",
    "print(f\"Test set ELBO per sample: {test_elbo_per_sample:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229bc65a",
   "metadata": {},
   "source": [
    "# Advanced Model Evaluation (Pick <u>One</u>)\n",
    "\n",
    "In this last part of the assignment, you should select **exactly one** of the following evaluation procedures.\n",
    "Your task is to implement your chosen evaluation fully and clearly, produce within your notebook at least **one informative visualization**, and **provide a 100500 word explanation** presenting:\n",
    "\n",
    "- **Motivation:** Why you selected this particular evaluation.\n",
    "- **Implementation:** How exactly your approach was implemented (with brief explanations for your visualization and choice of metrics).\n",
    "- **Insights:** What interesting facts, strengths, or weaknesses were revealed from applying this evaluation.\n",
    "\n",
    "-----\n",
    "\n",
    "Pick one of the following:\n",
    "\n",
    "4. **Posterior Weight Visualization**\n",
    "    * Examine posterior distributions of your parameters (coefficients of your polynomial regression).\n",
    "\n",
    "-----\n",
    "\n",
    "Out of the eight options, i thought it would be a good idea to evalueate the model using option 4. This is becuase i already learned a mean $\\mu$ and standard deviation $\\sigma$ for all of my weights. So visualizing those as a bar plot with error bars would be pretty easy to code with a simple matplotlib figure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d443f7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final mu: tensor([-1.6487,  0.0511,  0.1097, -0.0420])\n",
      "Final sigma: tensor([0.3292, 0.9991, 1.0003, 0.9970])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_mu = mu_param.detach()\n",
    "final_sigma = torch.exp(log_sigma_param).clamp(min=1e-3).detach()\n",
    "print(\"Final mu:\", final_mu)\n",
    "print(\"Final sigma:\", final_sigma)\n",
    "\n",
    "# Move variational parameters to CPU numpy\n",
    "weights_mu = final_mu.cpu().numpy()\n",
    "weights_sigma = final_sigma.cpu().numpy()\n",
    "param_names = [f\"w{i}\" for i in range(len(weights_mu))]\n",
    "\n",
    "# Plot posterior means with 1 std-error bars\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.bar(param_names, weights_mu, yerr=weights_sigma, capsize=4)\n",
    "plt.axhline(0, color='gray', linewidth=0.8)\n",
    "plt.xlabel('Parameter')\n",
    "plt.ylabel('Posterior mean  SD')\n",
    "plt.title('Posterior Weight Estimates with Uncertainty')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc92db0",
   "metadata": {},
   "source": [
    "After training and applying the first ELBo over epoch plot i detach the variational parameters which are `mu_param` and `log_sigma_param` and compute the sigma value. Using matplotlib i plotted a bar chart of $\\mu$ for each parameter, with vertical error bars of sigma and a horizontal line. This choice of +-1 sigma error bar reflect about a 65% credible interval under the approximate gaussian posterior, giving an immediate visual cue of uncertainty magnitude.\n",
    "\n",
    "The plot conclusively shows that only the bias term W_0 has a mu of -1.65 and sigma of 0.33 which means that the model is confident that the intercept is negative so it defaults toward the marginal spam rate. All other weights have means around 0 and sigma 1 which indicates that their posterior values collapse back to the standard normal prior. The model did NOT find enough signal in the three PCA features under the current ELBO weighting to  justify moving them. This means that the model is weak because the strong KL penalty is drowning the likelihood, so if we want to extract useful feature contributions, we will need to soften the KL term."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
