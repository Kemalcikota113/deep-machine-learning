{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4c7a23d",
   "metadata": {},
   "source": [
    "# Variational Inference: Spam Detection\n",
    "\n",
    "In this assignment, we will load the UCI SMS Spam Collection dataset.\n",
    "However, instead of using it directly, we will use fixed-size vector embeddings of the message.\n",
    "Those embeddings have already been produced and are provided here to you.\n",
    "\n",
    "\n",
    "## Setting the Scene\n",
    "\n",
    "The goal of this assignment to go beyond a traditional classifier and apply variational methods.\n",
    "This means that we will train a model using some parameters over the distribution of which we have a prior belief.\n",
    "Our prior belief here is actually that each and every parameter independently follows a standard normal distribution.\n",
    "\n",
    "\n",
    "During optimization, we will draw sets of parameters from our variational distribution $q_{\\phi}$.\n",
    "Drawing these parameters needs to be done using the reparameterization trick, so that we can add noise, i.e., $w_i=\\mu_i+\\sigma_i\\cdot\\epsilon$.\n",
    "The assignment also poses one or the other question (clearly marked), you need to provide your answer directly after.\n",
    "\n",
    "\n",
    "The assignment has some blank spots for you to fill out (but you can customize your implementation to your liking).\n",
    "Training should be done using stochastic gradient descend, either using manual gradient updates or using an optimizer.\n",
    "\n",
    "You should use autodiff-capabilities to compute gradients.\n",
    "It is recommended to use, for example, JAX or PyTorch.\n",
    "We recommend the latter, using it in a functional way (i.e., using `torch.func.jacrev`).\n",
    "The blanks left in this assignment and their type hints assume PyTorch.\n",
    "There are some cells with quick tests/sanity-checks, that you are free to remove, especially if they do not go along with how you chose to implement your solution.\n",
    "\n",
    "\n",
    "At the end of the assignment, after training, you need to pick one advanced method of evaluation.\n",
    "We are not interested in traditional metrics here (e.g., accuracy, Kappa, F1, etc.; although you are welcome to show those).\n",
    "Rather, we want to exploit the variational nature of the model here and show something more interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0cb7f4",
   "metadata": {},
   "source": [
    "# Load the Data\n",
    "\n",
    "You're variational model shall use no more than 15 components (aim for ~5 or fewer).\n",
    "You'll have to apply a dimensionality reduction.\n",
    "\n",
    "Sentence Embeddings were created in two ways:\n",
    "\n",
    "1. (Recommended) Using [ALBERT XLarge v2](https://huggingface.co/albert/albert-xlarge-v2) (`albert-xlarge-v2`). Dim $=2,048$.\n",
    "2. Using [English word vectors](https://fasttext.cc/docs/en/english-vectors.html) from `wiki-news-300d-1M` using `fasttext`. Dim $=300$.\n",
    "\n",
    "In either case, the embeddings were averaged along the sequence dimension to produce fixed-size vectors.\n",
    "In the provided dataset, the messages are retained.\n",
    "This is useful should you choose a qualitative evaluation.\n",
    "\n",
    "The labels have already been converted to floats: 0.0=ham, 1.0=spam.\n",
    "The default example below shows a spam message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "8da9d2e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5572, 2048),\n",
       " (5572,),\n",
       " 5572,\n",
       " np.str_(\"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\"),\n",
       " np.int64(1))"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modify only the file you wish to load.\n",
    "# Each data file contains 3 keys: X, y, msg\n",
    "import numpy as np\n",
    "\n",
    "data = np.load(file='2048d_sms_spam_albert-xlarge-v2.npz')\n",
    "# data = np.load(file='300d_sms_spam_fasttext_pca.npz')\n",
    "\n",
    "X: np.ndarray; Y: np.ndarray\n",
    "X, Y, msg = data.get('X'), data.get('y'), data.get('msg')\n",
    "X.shape, Y.shape, len(msg), msg[2], Y[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aeb3353",
   "metadata": {},
   "source": [
    "# Define the Model\n",
    "\n",
    "Our model is to be a polynomial with degree corresponding to the number of components chosen for the PCA.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    z=w_0+w_1\\cdot x_1+w_2x_2^2+\\dots+w_nx_n^n.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We will perform a **binary** classification problem, using the binary cross-entropy (CE).\n",
    "CE has a range of $[0,\\infty)$.\n",
    "The better the predictions of our model, the lower the CE.\n",
    "\n",
    "\n",
    "Note that binary CE requires our predictions to be in the range $[0,1]$.\n",
    "Therefore, we will have to pass its raw outputs (\"logits\") through the Sigmoid function.\n",
    "This makes our model a **logistic** classifier.\n",
    "The Sigmoid function is defined as $s(x)=\\frac{1}{1+e^{-z}}$.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    p_i&=s(z_i),\\;\\text{convert our raw predictions to probabilities, then:}\n",
    "    \\\\[1ex]\n",
    "    \\hat{y}_i&\\sim\\text{Bernoulli}(p_i).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "Since our predicted $\\hat{y}$ will follow a Bernoulli distribution, we can directy use its likelihood function.\n",
    "Note that maximizing the Bernoulli likelihood is equivalent of minimizing the binary CE!\n",
    "The Bernoulli distribution is parameterized by a single parameter, **$p$**.\n",
    "\n",
    "\n",
    "However, in our context, $p$ is unknown.\n",
    "In order not to confuse the Bernoulli distribution's parameter $p$ with anything, in the following, we have substituted it with $s(z_i)$.\n",
    "We will optimize for it, so that the output of our logisitic classifier becomes $p$.\n",
    "The likelihood (for the prediction $\\hat{y}_i$ of a single observation and its label $y_i$) then becomes:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    p(\\mathbf{y}|\\mathbf{X},\\mathbf{w})&=\\prod_i\\,s(z_i)^{y_i}\\cdot(1-s(z_i))^{1-y_i},\n",
    "    \\\\\n",
    "    \\log{(p(\\mathbf{y}|\\mathbf{X},\\mathbf{w}))}&=\\sum_i\\,\\left[\\log{(s(z_i)^{y_i})}+\\log{((1-s(z_i))^{1-y_i})}\\right],\n",
    "    \\\\\n",
    "    &=\\sum_i\\,\\left[y_i\\cdot\\log{(s(z_i))} + (1-y_i)\\cdot\\log{(1-s(z_i))}\\right].\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "* Our **prior** belief is that each parameter follows a standard normal distribution, i.e., $w_j\\sim\\mathcal{N}(0,1)$.\n",
    "* For our variational distribution, use a mean-field approximation of standard (independent) normals, too.\n",
    "\n",
    "\n",
    "**Question**: Conceptually, how does the log-likelihood $p(y|x,w)$ compute its result given a single $n$-dimensional observation under the assumption of independent dimensions?\n",
    "\n",
    "**Answer**: In case of independent dimensions, the (log-)likelihood across all dimensions is multiplied (summed)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8eaf94",
   "metadata": {},
   "source": [
    "# Implement the Model\n",
    "\n",
    "Here, you are encouraged to use a library/framework like PyTorch or JAX and esp. functionality for automatic differentiation to compute gradients.\n",
    "You may also import functions like `sigmoid` or `vmap` for vectorized operations.\n",
    "\n",
    "It is recommended to implement vectorized versions of your required functions (i.e., batch-processing).\n",
    "\n",
    "It is preferable to use type-hints for your functions.\n",
    "Furthermore, you can write better code by inserting assertions (e.g., for dimensionality or other sanity-checks).\n",
    "Please use Python-style comments like in the following:\n",
    "\n",
    "```python\n",
    "def func(w0: float, x: float) -> float:\n",
    "    \"\"\"\n",
    "    Function to compute a multiple of x.\n",
    "    \"\"\"\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc070fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "def model(w: Tensor, x: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Takes the coefficients w, the observations, computes the polynomial and\n",
    "    returns probabilities.\n",
    "    \"\"\"\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a06c5b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7839, 0.7999, 0.7693, 0.8770])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity-check, we should get 4 outputs if the model is correctly vectorized!\n",
    "temp = model(w=torch.rand(size=(1,6)), x=torch.rand(size=(4,5)))\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cd167f",
   "metadata": {},
   "source": [
    "## The Likelihood function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c3d50b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_lik(y_true: Tensor, y_hat: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    For one or more observations, where y_true is the true label (0 or 1)\n",
    "    and y_hat contains predicted probabilities [0,1], computes the (log)\n",
    "    likelihood.\n",
    "    \"\"\"\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9f5f9ac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-3.9837)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity-check, should be a single element here:\n",
    "log_lik(y_true=torch.tensor(data=[1,0,1,0], dtype=torch.float), y_hat=temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28d602e",
   "metadata": {},
   "source": [
    "# The Evidence Lower BOund (ELBO)\n",
    "\n",
    "Optimizing the ELBO (maximization) is the same as minimizing the KL divergence.\n",
    "Here, the students shall implement a **Monte Carlo** approximation.\n",
    "\n",
    "According to the slides, this is how it's done:\n",
    "\n",
    "1. Sample from approximate posterior distribution $q_{\\phi}(\\theta)$.\n",
    "    * Direct sampling methods should be used (we have a well-defined mean-field approximation here).\n",
    "    * Apply reparameterization trick to train the model. Note: Do **not** use amortized variational inference here.\n",
    "2. Estimate the ELBO using stochastic gradient-based optimization.\n",
    "\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911c504c",
   "metadata": {},
   "source": [
    "Remember that the Bayesian framework tells us something about the *model*.\n",
    "The variational approach allows us to **empirically** estimate the overall goodness of fit of our model.\n",
    "\n",
    "In order to do that sufficiently well, we need more than just point estimates.\n",
    "Recall that we do **not** attempt to find some best point estimates for our data, but rather a distribution over them.\n",
    "In order for that to work well, we need to test many different parameter constellations and average over those results.\n",
    "In other words, we need to draw many different sets of possible variational distribution parameters and check how well these allow our model, on average, to predict the constant observations.\n",
    "\n",
    "\n",
    "-------\n",
    "\n",
    "Recall the definition of the ELBO:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\text{ELBO}&=\\mathbb{E}_{q_{\\phi}(\\mathbf{w})}\\left[\\log{(p(\\mathbf{y}|\\mathbf{X},\\mathbf{w}))}\\right]-D_{\\text{KL}}\\left(q_{\\phi}(\\mathbf{w})\\|p(\\mathbf{w})\\right).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "-------\n",
    "\n",
    "For simplicity here, assume there will be the following \"loops\":\n",
    "\n",
    "1. Outermost loop is governed by the epoch ($E$).\n",
    "2. The next loop is over the stochastic mini-batches of observations (batches of size $N$).\n",
    "    * For each batch, you draw $w^{(s)}\\in 1\\dots S$ **new** different sets of parameter configurations from your variational distribution.\n",
    "    * Here, you'll be using the reparameterization trick.\n",
    "3. Loop over $S$:\n",
    "    * (a) For each configuration $s_i$, you will have to multiply (sum) the (log) likelihood of each observation under the current likelihood function (as parameterized by $w^{(s)}$).\n",
    "    * (b) Next, calculate the KL-divergence between our approximate posterior (the mean-field approximation of independent Gaussians) and our prior (which is a diagonal standard normal distribution). **Attention**:\n",
    "        * You use either, the **analytical** or the **Monte Carlo** approximation of the KL-divergence. However, the analytical one is essentially *outside the expectation* (because it does not average over $w^{(s)}$), whereas the MC-approximation should perhaps be an addend/subtrahend to (a).\n",
    "        * In effect, the analytical KL-divergence is calculated only **once** per batch, using the **current** variational parameters (i.e., as they were after the last optimizer's step or after initialization for first step).\n",
    "    * Calculate (a) - (b), according to previous remark.\n",
    "4. Average the results from step 3 (considering the remark about analytical/MC version of the KL-divergence). It needs to be an average because the ELBO is an expectation (a weighted mean) over all possible realizations of $s_i\\in S$. For each individual batch, you have now an average idea of:\n",
    "    * The *expected* data likelihood.\n",
    "    * How strongly your prior and approximate posterior diverge from one another.\n",
    "5. Do not accumulate results across batches at this point.\n",
    "    * It is better to compute a gradient for each batch and apply parameter updates. Frequent, incremental updates work better in practice.\n",
    "\n",
    "\n",
    "**Notes**:\n",
    "* As $S$ approaches $\\infty$, the MC-approximation of the KL-divergence will be equal to the analytical solution.\n",
    "* For the MC-approximation, a good $S$ is perhaps $50-500$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3a1640",
   "metadata": {},
   "source": [
    "# Putting it all together\n",
    "\n",
    "Let's implement the \"loops\" from above.\n",
    "We will create stochastic mini-batches of our data to compute gradients on.\n",
    "For now, we will not implement a loop for epochs.\n",
    "\n",
    "The ELBO for a list of parameters sets $S$ and a mini-batch of length $N$ is defined as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\text{ELBO}(\\phi)&\\approx\\left(\\frac{1}{S}\\sum_{s=1}^{S}\\,\\underbrace{\\left[\\sum_{i=1}^{N}\\,\\log{(p(y_i|x_i,w^{(s)}))}\\right]}_{\\text{log-sum of i.i.d. observations}}\\right)-\\underbrace{D_{\\text{KL}}(q_{\\mu,\\sigma}(w)\\|p(w))}_{\\text{using the current $\\mu,\\sigma$}}.\n",
    "    \\\\[4em]\n",
    "    \\text{Also note that}&\\text{ we can use Monte Carlo to approximate the KL-divergence:}\\nonumber\n",
    "    \\\\[1em]\n",
    "    D_{\\text{KL}}(q_{\\mu,\\sigma}(w)\\|p(w))&=\\mathbb{E}_{w\\sim q_{\\mu,\\sigma}(w)}\\left[\\log{\\left(\\frac{q_{\\mu,\\sigma}(w)}{p(w)}\\right)}\\right],\n",
    "    \\\\[1em]\n",
    "    &=\\frac{1}{S}\\sum_{s=1}^{S}\\,\\left[\\log{\\left(q_{\\mu,\\sigma}\\left(w^{(s)}\\right)\\right)}-\\log{\\left(p\\left(w^{(s)}\\right)\\right)}\\right].\n",
    "    \\\\[2em]\n",
    "    \\text{Also note that}&\\text{ the KL-divergence between two normals has an analytical solution:}\\nonumber\n",
    "    \\\\[1em]\n",
    "    D_{\\text{KL}}(q_{\\mu,\\sigma}(w)\\|\\mathcal{N}(0,I))&=\\frac{1}{2}\\sum_{j=0}\\,\\left(\\mu_j^2+\\sigma_j^2-1-\\log{\\left(\\sigma_j^2\\right)}\\right).\n",
    "    \\\\[2em]\n",
    "    \\text{Also note that}&\\text{ the ELBO including the MC-approximation of the KL-divergence is:}\\nonumber\n",
    "    \\\\[1em]\n",
    "    \\text{ELBO}(\\phi)&\\approx\\frac{1}{S}\\sum_{s=1}^{S}\\,\\underbrace{\\left[\\sum_{i=1}^{N}\\,\\log{(p(y_i|x_i,w^{(s)}))}\\right]}_{\\text{log-sum of i.i.d. observations}}-\\underbrace{\\left[\\log{\\left(q_{\\mu,\\sigma}\\left(w^{(s)}\\right)\\right)}-\\log{\\left(p\\left(w^{(s)}\\right)\\right)}\\right]}_{\\text{MC-approx. of KL-divergence}}.\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc32b04f",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "\n",
    "Here, implement everything you need.\n",
    "\n",
    "## Data Preparation and Dimensionality Reduction\n",
    "\n",
    "Start by reducing the dimensionality of your data.\n",
    "Choose a combination of dataset and polynomial degree that is not too low and not too high.\n",
    "Aim for at least 30% explained variance and at most 15 components.\n",
    "\n",
    "Report the (sum of the) explained variance before you proceed.\n",
    "\n",
    "\n",
    "Remember the basics: splitting, randomness, scaling, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de2e774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained total variance by 4 dimensions: 0.469280\n"
     ]
    }
   ],
   "source": [
    "POLY_DEGREE = 4\n",
    "\n",
    "# TODO: Perform dimensionality reduction and prepare all data as needed.\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17a1521",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Below you'll find some prototypes (fill in the blanks).\n",
    "Again, this is just a suggestion, you can come up with your own implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a1cbe05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ELBO_expected_data_likelihood(y_true: Tensor, x: Tensor, W: Tensor) -> Tensor:\n",
    "    \"\"\"A function to calculate the first term of the ELBO.\"\"\"\n",
    "    ...\n",
    "\n",
    "\n",
    "def ELBO_KL_divergence_analytical(mu: Tensor, sigma: Tensor) -> Tensor:\n",
    "    \"\"\"The analytical version of the KL divergence.\"\"\"\n",
    "    ...\n",
    "\n",
    "\n",
    "def ELBO_KL_divergence_Monte_Carlo(W: Tensor, mu: Tensor, sigma: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    The Monte Carlo approximation of the KL divergence.\n",
    "    You don't need to implement both, one is enough.\n",
    "    \"\"\"\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca5d5e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "NUM_VARIATIONAL_SETS = 50\n",
    "KL_DIV_TYPE = Literal['analytical', 'montecarlo']\n",
    "\n",
    "\n",
    "def ELBO(use_mu: Tensor, use_sigma: Tensor, y_true: Tensor, obs: Tensor, variational_params_noise: Tensor, kl: KL_DIV_TYPE='analytical', return_exp_data_lik: bool=False, return_kl_div: bool=False) -> Tensor|tuple[Tensor, ...]:\n",
    "    \"\"\"\n",
    "    Convenience function that uses the current variational parameters,\n",
    "    applies the reparameterization trick, and computes the complete\n",
    "    ELBO. The result of this function shall be maximized.\n",
    "    \"\"\"\n",
    "    ...\n",
    "\n",
    "\n",
    "# TODO: Define/create the gradient of the ELBO function.\n",
    "ELBO_grad = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d6ea3a",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Train your model until convergence.\n",
    "Choose a number of iterations, batch-size, and learning rate that make sense in your scenario.\n",
    "Do not perform a grid search or other hyperparameter optimization.\n",
    "Instead, manually find some good working parameters and make your final solution just use these.\n",
    "\n",
    "\n",
    "* **Plot** the training curve (i.e., plot the history for each component of the ELBO).\n",
    "* **Print** the final parameters for your variational distribution after optimization.\n",
    "* **Evaluate** the ELBO on the holdout dataset. Is it close? You could also do this during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f128226f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement as however you require this!\n",
    "# TODO: Make sure to use the finally found optimal parameters once you submit your solution.\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 0.0001\n",
    "USE_KL_TYPE: KL_DIV_TYPE = 'analytical'\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229bc65a",
   "metadata": {},
   "source": [
    "# Advanced Model Evaluation (Pick <u>One</u>)\n",
    "\n",
    "In this last part of the assignment, you should select **exactly one** of the following evaluation procedures.\n",
    "Your task is to implement your chosen evaluation fully and clearly, produce within your notebook at least **one informative visualization**, and **provide a 100–500 word explanation** presenting:\n",
    "\n",
    "- **Motivation:** Why you selected this particular evaluation.\n",
    "- **Implementation:** How exactly your approach was implemented (with brief explanations for your visualization and choice of metrics).\n",
    "- **Insights:** What interesting facts, strengths, or weaknesses were revealed from applying this evaluation.\n",
    "\n",
    "-----\n",
    "\n",
    "Pick one of the following:\n",
    "\n",
    "\n",
    "1. **Visualization of Learned Low-dimensional Representations**\n",
    "    * Visualize embeddings from your variational polynomial model using dimensionality reduction (e.g., PCA, t-SNE, UMAP).\n",
    "    * Color embeddings by posterior uncertainty estimates or predictions.\n",
    "    * This illustrates nicely how your model's uncertainty varies across regions of your feature space.\n",
    "2. **Posterior Predictive Checks (PPC) (Visualizations of uncertainty)**\n",
    "    * Posterior predictive checks evaluate your model by extending its sampled posterior parameters forward into the data space, then comparing the simulated data (from the posterior) with the real observed data.\n",
    "3. **Evaluation of the ELBO on Hold-out Data and Bayesian (Probabilistic) Performance Metrics**\n",
    "    * Evaluate the quality of your approximate posterior by evaluating ELBO on the hold-out set.\n",
    "    * Bayesian performance metrics, for example: Log Predictive Density (LPD) and Bayesian Information Criterion (BIC)\n",
    "4. **Posterior Weight Visualization**\n",
    "    * Examine posterior distributions of your parameters (coefficients of your polynomial regression).\n",
    "5. **Out-of-Distribution (OoD) Detection Capability incl. Likelihood-Based Evaluation (Density Estimation Quality)**\n",
    "    * E.g., generate data points far away from the hold-out dataset and evaluate likelihood, uncertainty, etc.\n",
    "    * Compare computed likelihood of the train- and hold-out dataset.\n",
    "6. **Expected Calibration Error (ECE) and Calibration Curves (Reliability Diagrams)**\n",
    "    * Expected Calibration Error numerically summarizes the calibration curves, giving you a single statistic to quantify how good or poor calibration of a model is.\n",
    "7. **Latent Space Smoothness and Structure**\n",
    "    * Generate intermediate points (linear interpolation) between latent variables from spam and non-spam messages and decode these embeddings. Good generative models produce semantically smooth interpolations.\n",
    "8. **Qualitative Evaluation of a few messages**\n",
    "    * Pick about ~10 messages from the holdout dataset. Among those 10, you should have some that are obviously spam, some that are ambiguous, and some that are clearly ham.\n",
    "    * Evaluate the model's predictions and uncertainty about these predictions. Is there a scheme that materializes here?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "4dv661",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
