{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df3727e4",
   "metadata": {},
   "source": [
    "# Assignment: Foundations of Bayesâ€™ theorem\n",
    "\n",
    "Fill out the blanks as per the instructions below.\n",
    "\n",
    "This assignment uses type hints, so make sure to stick to those.\n",
    "\n",
    "Whenever you need to fill in a blank, we used Python's ellipsis (`...`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7addde5",
   "metadata": {},
   "source": [
    "# Part 1 (A): Bayes' theorem with discrete random variables\n",
    "\n",
    "Here, we assume a discrete prior $P(\\theta)$, as well as a discrete probability distribution over a few i.i.d. observations.\n",
    "\n",
    "The goal is to manually implement functionals for computing marginal and conditional likelihoods/probability densities.\n",
    "\n",
    "You need to show that the posterior probability $P(\\theta|Y)$ is a proper probability mass function.\n",
    "Choose a different number of parameters and observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "01e7ecda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define our parameters theta and their probabilities (our prior belief):\n",
    "# A handful of thetas is enough.\n",
    "theta: list[int] = [1,2,3]\n",
    "theta_probs: list[float] = [0.2, 0.5, 0.3]   # sums to 1\n",
    "\n",
    "# Here are our observations Y (don't change!):\n",
    "Y_obs = [0.5, 1.2]\n",
    "\n",
    "# Instead of assuming some (parameterized) distribution,\n",
    "# we hardcode the conditional likelihoods of Y given some theta.\n",
    "# Note that P(Y|\\theta) is a likelihood, so it does not represent\n",
    "# (necessarily) a valid probability density (i.e., values for each\n",
    "# \\theta do not necessarily have to sum to 1).\n",
    "P_Y_given_theta: dict[int, dict[float, float]] = {\n",
    "    1: {0.5: 0.10, 1.2: 0.40},\n",
    "    2: {0.5: 0.30, 1.2: 0.20},\n",
    "    3: {0.5: 0.05, 1.2: 0.60},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bfcd27",
   "metadata": {},
   "source": [
    "## Define PMFs\n",
    "\n",
    "For convenience, we define the PMFs for $\\theta$ and $Y$ explicitly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d9e04636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't change!\n",
    "def P_theta(val: float) -> float:\n",
    "    assert val in theta\n",
    "    idx = theta.index(val)\n",
    "    return theta_probs[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019a8211",
   "metadata": {},
   "source": [
    "## Define Functions\n",
    "\n",
    "for the likelihood $P(Y|\\theta)$, the prior $P(\\theta)$, and the evidence $P(Y)$.\n",
    "\n",
    "Recall that the evidence:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    P(Y)&=\\sum_i\\,P(Y|\\theta_i)\\times P(\\theta_i)\\nonumber.\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3e4c26f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Likelihood, P(Y|\\theta), now explicitly from our discrete definition:\n",
    "def likelihood(Y: list[float], t: float) -> float:\n",
    "    lik = 1.0\n",
    "    for y in Y:\n",
    "        lik *= P_Y_given_theta[t][y]\n",
    "    return lik\n",
    "\n",
    "# The Evidence (in this assignment, it *is* computable):\n",
    "def P_Y(Y: list[float]) -> float:\n",
    "    temp = 0.0\n",
    "    for t in theta:\n",
    "        temp += P_theta(t) * likelihood(Y, t)\n",
    "    return temp\n",
    "\n",
    "# The posterior:\n",
    "def P_theta_given_Y(t: float, Y: list[float]) -> float:\n",
    "    temp = P_theta(t) * likelihood(Y, t) / P_Y(Y)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1da3f133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.17021, 0.6383, 0.19149]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Don't change!\n",
    "posterior_probs = [round(P_theta_given_Y(t=t, Y=Y_obs), ndigits=5) for t in theta]\n",
    "posterior_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3a3298e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Don't change! The result here needs to be ~1.0!\n",
    "print(sum(posterior_probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166fe812",
   "metadata": {},
   "source": [
    "I add this markdown/LaTeX cell just to mathematically prove that the result that i got is true:\n",
    "\n",
    "We can compute each likelihood:\n",
    "\n",
    "$$P(Y | \\theta = 1) = 0.1 * 0.4 = 0.04$$\n",
    "$$P(Y | \\theta = 2) = 0.3 * 0.2 = 0.06$$\n",
    "$$P(Y | \\theta = 3) = 0.05 * 0.6 = 0.03$$\n",
    "\n",
    "We can then multiply the theese likelihoods with the probabilites of each outcome:\n",
    "\n",
    "$$0.2 * 0.04 = 0.008$$\n",
    "$$0.5 * 0.06 = 0.03$$\n",
    "$$0.3 * 0.03 = 0.009$$\n",
    "\n",
    "We can then sum theee together in order to get:\n",
    "\n",
    "$$P(Y) = 0.008 + 0.03 + 0.009 = 0.047$$\n",
    "\n",
    "Now we can finally compute the conditional probabilities, which means that we can compute the probability of a given theta value given that Y holds.\n",
    "\n",
    "$$P(\\theta = 1 | Y) = \\frac{0.08}{0.047} = 0.17021$$\n",
    "$$P(\\theta = 2 | Y) = \\frac{0.03}{0.047} = 0.63830$$\n",
    "$$P(\\theta = 3 | Y) = \\frac{0.009}{0.047} = 0.19149$$\n",
    "\n",
    "This makes sence because of two reasons. Firstly we can see that the higher theta probabilites give higher conditional probabilites and this makes sence because the numerator in the conditional fraction will be higher but Y is constant. And the second reason is that when we sum all of theese together, they add up to 1. This is becuse when we add all outcomes together we get a summed probability of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32560e80",
   "metadata": {},
   "source": [
    "# Part 1(B): Bayes' theorem with continuous random variables\n",
    "\n",
    "-------------------------\n",
    "\n",
    "Now, we change our model a bit.\n",
    "Instead of assuming a small discrete set of possible values for $\\theta$, we will assume that this parameter follows a standard normal distribution.\n",
    "\n",
    "For our actual model, we will assume another normal distribution, where the standard deviation (scale) is fixed at $\\frac{3}{2}$ and the mean is set to $\\theta$: $N\\sim(\\mu=\\theta,\\sigma=\\frac{3}{2})$.\n",
    "\n",
    "We will re-use the previous observations.\n",
    "\n",
    "\n",
    "The evidence, defined continuously:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    P(Y)=\\int_{\\theta}\\,P(Y|t)\\times P(t)\\,d\\theta\\nonumber.\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8b3c74d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.distributions import norm\n",
    "\n",
    "# Our prior:\n",
    "def P_theta_continuous(val: float) -> float:\n",
    "    # Use norm.pdf() to compute this.\n",
    "    norm_pdf = norm.pdf(val, loc=0, scale=1).item()\n",
    "    return norm_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "82180d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.integrate import quad\n",
    "from typing import final\n",
    "\n",
    "\n",
    "# Realistically, our bounds could be -10,10 (or similar), but\n",
    "# scipy's quad allows to use infinity, so we'll use that, as\n",
    "# it's also closer to how we would formulate this mathematically.\n",
    "a, b = -np.inf, np.inf\n",
    "\n",
    "\n",
    "# Our model prototype that takes a single scale parameter that\n",
    "# will be held fixed for any subsequent likelihood computations.\n",
    "@final\n",
    "class Model:\n",
    "    \"\"\"Keep using this model class as-is, no need to change it.\"\"\"\n",
    "    def __init__(self, scale: float):\n",
    "        self.scale = scale\n",
    "    \n",
    "    def likelihood(self, x: float, mean: float) -> float:\n",
    "        return norm.pdf(x=x, loc=mean, scale=self.scale).item()\n",
    "    \n",
    "\n",
    "def likelihood_continuous(Y: list[float], t: float, model: Model) -> float:\n",
    "    likelihood = 1.0\n",
    "    for y in Y:\n",
    "        likelihood *= model.likelihood(x=y, mean=t)\n",
    "    return likelihood\n",
    "\n",
    "\n",
    "def integrand_function(t, Y, model): # helper function for P_Y_continuous\n",
    "    return P_theta_continuous(t) * likelihood_continuous(Y=Y, t=t, model=model)\n",
    "\n",
    "# for this i had to create a function integrand_function becuase the quad function expects another function in its args and i dont like lambda expressions. I hope this is OK.\n",
    "def P_Y_continuous(Y: list[float], model: Model) -> float: \n",
    "    \"\"\"Use quad() to integrate.\"\"\"\n",
    "    result = quad(func=integrand_function, a=a, b=b, args=(Y, model))[0]\n",
    "    return result\n",
    "\n",
    "\n",
    "def P_theta_given_Y_continuous(t: float, Y: list[float], evidence: float, model: Model) -> float:\n",
    "    temp = P_theta_continuous(t) * likelihood_continuous(Y=Y, t=t, model=model) / evidence\n",
    "    return temp\n",
    "\n",
    "# The goal of this function is to assert that our posterior is\n",
    "# a valid probability density that sums/integrates to 1.\n",
    "def P_theta_given_Y_continuous_integral(Y: list[float], model: Model) -> float:\n",
    "    evidence = P_Y_continuous(Y=Y, model=model)\n",
    "    func = lambda t: P_theta_given_Y_continuous(Y=Y, t=t, model=model, evidence=evidence)\n",
    "    return quad(func=func, a=a, b=b)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a73924",
   "metadata": {},
   "source": [
    "Now we show the amount of evidence, as well as that our posterior integrates to $\\approx1$:\n",
    "Also, we show the amount of (log-)evidence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b8c34f27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.1912461104301157, 0.9999999999999991)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Don't change. Prints the log-evidence, as well as its integral (should be ~1.0).\n",
    "from math import log\n",
    "use_model = Model(scale=1.5)\n",
    "\n",
    "log(P_Y_continuous(Y=Y_obs, model=use_model)),\\\n",
    "P_theta_given_Y_continuous_integral(Y=Y_obs, model=use_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cd0b42",
   "metadata": {},
   "source": [
    "### Find and use a better model\n",
    "\n",
    "Recall that our observations were fixed at $[0.5, 1.2]$ and we assumed our model would be a normal distribution with standard deviation $\\sigma=\\frac{3}{2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "86018ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.35, 0.85)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_obs_arr = np.array(Y_obs)\n",
    "Y_obs_arr.std().item(), Y_obs_arr.mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb28220",
   "metadata": {},
   "source": [
    "However, we know that the standard deviation should likely be smaller to accommodate our data better.\n",
    "What we want to show here, is that a better model (here: same as previous but with a fixed standard deviation closer to $0.35$) produces a larger **evidence**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1f13fb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "# Use 'minimize_scalar' to find some optimal solution\n",
    "optimal_scale = minimize_scalar(lambda s: -log(P_Y_continuous(Y_obs, Model(scale=s))), bounds=(1e-6, 10.0), method='bounded').x\n",
    " # <- fill in the blank here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "31db941e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-2.360448901825615, 1.0)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Don't change! Prints the log-evidence, as well as its integral (should be ~1.0).\n",
    "better_model = Model(scale=optimal_scale)\n",
    "\n",
    "log(P_Y_continuous(Y=Y_obs, model=better_model)),\\\n",
    "P_theta_given_Y_continuous_integral(Y=Y_obs, model=better_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c891f1",
   "metadata": {},
   "source": [
    "# Short evaluation (write 1-2 sentences per):\n",
    "\n",
    "1. How has the (log-)evidence changed using the optimal scale?\n",
    "2. In Bayesian terms, what does this result mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1073b2ed",
   "metadata": {},
   "source": [
    "**Answers**:\n",
    "\n",
    "1. When we used a fixed scale of $\\sigma = 1.5$ we found that $log(P(Y | \\sigma)) = -3.1912461$. After optimizing the scale we found that $log(P(Y)) = -2.360489$. This means that the difference between the initial and the optimized log-evidence increased by about 0.83 units.\n",
    "\n",
    "2. The marginal likelihood (what is called 'evidence' in the code) $P(Y)$ measures how well the model predicts the observed data when it integrates over all possible $\\theta$. the fact that $log(P(Y))$ goes up means that the optimized noise scale explains the data better under that scaling assumption. The observed Y value are more probable by a factor of $exp(0.83)$ than under the original $\\sigma = 1.5$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
